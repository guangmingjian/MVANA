Dataset: NCI1,
Model Name: MVANA
net_params={'alpha': 0.2, 'beta': 0.0001, 'gcn_nums': 3, 'dropout': 0.1, 'd_kv': 32, 'gcn_dropout': 0.0, 'att_drop_ratio': 0.0, 'mask_rate': 0.1, 'graph_norm': True, 'sz_c': 4, 'gcn_h_dim': 128, 'g_name': 'GraphSAGE', 'cnn_out_ker': 18, 'hidden_kernel': 16, 'cnn_dropout': 0.0, 'is_cbn': False, 'is_em': False, 'MFeatype': 'MFM', 'in_channels': 37, 'out_channels': 2, 'device': 'cuda:1'}
train_config={'kf': 10, 'epochs': 300, 'batch_size': 256, 'seed': 8971, 'patience': 50, 'lr': 0.01, 'weight_decay': 1e-05}
MVANA(
  (mgl): MultiGCNLayers(
    (gcn_layer): ModuleList(
      (0): ModuleList(
        (0): SAGEConv(37, 32)
        (1): Dropout(p=0.0, inplace=False)
        (2): GraphSizeNorm()
        (3): BatchNorm(32)
        (4): ReLU()
        (5): SAGEConv(32, 32)
        (6): Dropout(p=0.0, inplace=False)
        (7): GraphSizeNorm()
        (8): BatchNorm(32)
        (9): ReLU()
        (10): SAGEConv(32, 32)
        (11): Dropout(p=0.0, inplace=False)
        (12): GraphSizeNorm()
        (13): BatchNorm(32)
        (14): ReLU()
      )
      (1): ModuleList(
        (0): SAGEConv(37, 32)
        (1): Dropout(p=0.0, inplace=False)
        (2): GraphSizeNorm()
        (3): BatchNorm(32)
        (4): ReLU()
        (5): SAGEConv(32, 32)
        (6): Dropout(p=0.0, inplace=False)
        (7): GraphSizeNorm()
        (8): BatchNorm(32)
        (9): ReLU()
        (10): SAGEConv(32, 32)
        (11): Dropout(p=0.0, inplace=False)
        (12): GraphSizeNorm()
        (13): BatchNorm(32)
        (14): ReLU()
      )
      (2): ModuleList(
        (0): SAGEConv(37, 32)
        (1): Dropout(p=0.0, inplace=False)
        (2): GraphSizeNorm()
        (3): BatchNorm(32)
        (4): ReLU()
        (5): SAGEConv(32, 32)
        (6): Dropout(p=0.0, inplace=False)
        (7): GraphSizeNorm()
        (8): BatchNorm(32)
        (9): ReLU()
        (10): SAGEConv(32, 32)
        (11): Dropout(p=0.0, inplace=False)
        (12): GraphSizeNorm()
        (13): BatchNorm(32)
        (14): ReLU()
      )
      (3): ModuleList(
        (0): SAGEConv(37, 32)
        (1): Dropout(p=0.0, inplace=False)
        (2): GraphSizeNorm()
        (3): BatchNorm(32)
        (4): ReLU()
        (5): SAGEConv(32, 32)
        (6): Dropout(p=0.0, inplace=False)
        (7): GraphSizeNorm()
        (8): BatchNorm(32)
        (9): ReLU()
        (10): SAGEConv(32, 32)
        (11): Dropout(p=0.0, inplace=False)
        (12): GraphSizeNorm()
        (13): BatchNorm(32)
        (14): ReLU()
      )
    )
    (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
  )
  (smus): ModuleList(
    (0): Attv2(
      (trans_lin): ModuleList(
        (0): Linear(in_features=64, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=32, bias=True)
      )
      (att_lin): Linear(in_features=64, out_features=1, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
    )
    (1): Attv2(
      (trans_lin): ModuleList(
        (0): Linear(in_features=64, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=32, bias=True)
      )
      (att_lin): Linear(in_features=64, out_features=1, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
    )
    (2): Attv2(
      (trans_lin): ModuleList(
        (0): Linear(in_features=64, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=32, bias=True)
      )
      (att_lin): Linear(in_features=64, out_features=1, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
    )
    (3): Attv2(
      (trans_lin): ModuleList(
        (0): Linear(in_features=64, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=32, bias=True)
      )
      (att_lin): Linear(in_features=64, out_features=1, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
    )
  )
  (vlr): VLRLayers(
    (k_fc): Linear(in_features=32, out_features=32, bias=True)
    (v_fc): Linear(in_features=32, out_features=32, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
  )
  (cnn_net): LeNet(
    (conv1): Conv2d(4, 16, kernel_size=(5, 5), stride=(1, 1))
    (conv2): Conv2d(16, 18, kernel_size=(5, 5), stride=(1, 1))
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (fc1): Linear(in_features=450, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
)

fold [0/10] is start!!
[01/0001] | train_loss:0.6762 val_acc:49.1484 val_loss:0.6872
model is saved at epoch 1!![01/0002] | train_loss:0.653 val_acc:50.8516 val_loss:0.6882
model is saved at epoch 2!![01/0003] | train_loss:0.6337 val_acc:58.6375 val_loss:0.6827
model is saved at epoch 3!![01/0004] | train_loss:0.6216 val_acc:53.2847 val_loss:0.6679
[01/0005] | train_loss:0.6021 val_acc:62.0438 val_loss:0.6348
model is saved at epoch 5!![01/0006] | train_loss:0.5893 val_acc:68.6131 val_loss:0.5924
model is saved at epoch 6!![01/0007] | train_loss:0.578 val_acc:69.3431 val_loss:0.588
model is saved at epoch 7!![01/0008] | train_loss:0.5638 val_acc:72.2628 val_loss:0.5485
model is saved at epoch 8!![01/0009] | train_loss:0.5583 val_acc:73.236 val_loss:0.5494
model is saved at epoch 9!![01/0010] | train_loss:0.5455 val_acc:73.236 val_loss:0.5502
[01/0011] | train_loss:0.537 val_acc:73.9659 val_loss:0.5306
model is saved at epoch 11!![01/0012] | train_loss:0.528 val_acc:73.7226 val_loss:0.5252
[01/0013] | train_loss:0.5272 val_acc:73.4793 val_loss:0.5155
[01/0014] | train_loss:0.5121 val_acc:74.9392 val_loss:0.5271
model is saved at epoch 14!![01/0015] | train_loss:0.5088 val_acc:74.6959 val_loss:0.5055
[01/0016] | train_loss:0.4968 val_acc:75.9124 val_loss:0.4924
model is saved at epoch 16!![01/0017] | train_loss:0.5013 val_acc:76.1557 val_loss:0.4929
model is saved at epoch 17!![01/0018] | train_loss:0.5032 val_acc:75.4258 val_loss:0.4961
[01/0019] | train_loss:0.4891 val_acc:74.4526 val_loss:0.4885
[01/0020] | train_loss:0.4929 val_acc:74.4526 val_loss:0.4897
[01/0021] | train_loss:0.4769 val_acc:77.6156 val_loss:0.4768
model is saved at epoch 21!![01/0022] | train_loss:0.4753 val_acc:76.8856 val_loss:0.4853
[01/0023] | train_loss:0.4672 val_acc:76.8856 val_loss:0.4781
[01/0024] | train_loss:0.4573 val_acc:76.8856 val_loss:0.4659
[01/0025] | train_loss:0.4558 val_acc:77.129 val_loss:0.515
[01/0026] | train_loss:0.462 val_acc:76.1557 val_loss:0.4794
[01/0027] | train_loss:0.4535 val_acc:77.129 val_loss:0.465
[01/0028] | train_loss:0.4603 val_acc:78.3455 val_loss:0.5049
model is saved at epoch 28!![01/0029] | train_loss:0.4405 val_acc:77.3723 val_loss:0.4724
[01/0030] | train_loss:0.4433 val_acc:77.6156 val_loss:0.4773
[01/0031] | train_loss:0.426 val_acc:77.6156 val_loss:0.4609
[01/0032] | train_loss:0.4307 val_acc:77.8589 val_loss:0.4719
[01/0033] | train_loss:0.4301 val_acc:78.5888 val_loss:0.4515
model is saved at epoch 33!![01/0034] | train_loss:0.4151 val_acc:79.0754 val_loss:0.4575
model is saved at epoch 34!![01/0035] | train_loss:0.4205 val_acc:77.129 val_loss:0.4535
[01/0036] | train_loss:0.4134 val_acc:77.6156 val_loss:0.4612
[01/0037] | train_loss:0.4142 val_acc:80.292 val_loss:0.4525
model is saved at epoch 37!![01/0038] | train_loss:0.4137 val_acc:78.3455 val_loss:0.4615
[01/0039] | train_loss:0.414 val_acc:79.3187 val_loss:0.4506
[01/0040] | train_loss:0.4106 val_acc:80.7786 val_loss:0.4919
model is saved at epoch 40!![01/0041] | train_loss:0.4076 val_acc:79.3187 val_loss:0.4498
[01/0042] | train_loss:0.3992 val_acc:81.0219 val_loss:0.4556
model is saved at epoch 42!![01/0043] | train_loss:0.3887 val_acc:80.292 val_loss:0.4437
[01/0044] | train_loss:0.3984 val_acc:79.0754 val_loss:0.4482
[01/0045] | train_loss:0.3951 val_acc:80.292 val_loss:0.4427
[01/0046] | train_loss:0.3746 val_acc:79.0754 val_loss:0.4529
[01/0047] | train_loss:0.3794 val_acc:81.5085 val_loss:0.4476
model is saved at epoch 47!![01/0048] | train_loss:0.399 val_acc:79.8054 val_loss:0.4319
[01/0049] | train_loss:0.3862 val_acc:79.0754 val_loss:0.4419
[01/0050] | train_loss:0.3872 val_acc:80.5353 val_loss:0.4541
[01/0051] | train_loss:0.3697 val_acc:80.0487 val_loss:0.4426
[01/0052] | train_loss:0.3739 val_acc:78.5888 val_loss:0.4505
[01/0053] | train_loss:0.3818 val_acc:80.7786 val_loss:0.4397
[01/0054] | train_loss:0.381 val_acc:79.3187 val_loss:0.4415
[01/0055] | train_loss:0.3703 val_acc:82.9684 val_loss:0.442
model is saved at epoch 55!![01/0056] | train_loss:0.3574 val_acc:80.0487 val_loss:0.4631
[01/0057] | train_loss:0.3678 val_acc:78.5888 val_loss:0.5044
[01/0058] | train_loss:0.3721 val_acc:81.0219 val_loss:0.4665
[01/0059] | train_loss:0.351 val_acc:81.7518 val_loss:0.456
[01/0060] | train_loss:0.3585 val_acc:81.2652 val_loss:0.4318
[01/0061] | train_loss:0.3553 val_acc:80.7786 val_loss:0.4502
[01/0062] | train_loss:0.3533 val_acc:79.8054 val_loss:0.4489
[01/0063] | train_loss:0.3647 val_acc:80.5353 val_loss:0.4461
[01/0064] | train_loss:0.3465 val_acc:78.5888 val_loss:0.5023
[01/0065] | train_loss:0.3546 val_acc:81.2652 val_loss:0.4427
[01/0066] | train_loss:0.3512 val_acc:80.0487 val_loss:0.4658
[01/0067] | train_loss:0.3405 val_acc:80.5353 val_loss:0.4717
[01/0068] | train_loss:0.3374 val_acc:81.5085 val_loss:0.4384
[01/0069] | train_loss:0.3414 val_acc:82.9684 val_loss:0.433
[01/0070] | train_loss:0.3401 val_acc:80.5353 val_loss:0.4392
[01/0071] | train_loss:0.3377 val_acc:81.2652 val_loss:0.4609
[01/0072] | train_loss:0.3314 val_acc:80.7786 val_loss:0.4674
[01/0073] | train_loss:0.3305 val_acc:80.0487 val_loss:0.478
[01/0074] | train_loss:0.3118 val_acc:80.0487 val_loss:0.4734
[01/0075] | train_loss:0.3153 val_acc:80.7786 val_loss:0.4732
[01/0076] | train_loss:0.3226 val_acc:82.2384 val_loss:0.4751
[01/0077] | train_loss:0.3293 val_acc:81.5085 val_loss:0.4512
[01/0078] | train_loss:0.3295 val_acc:82.7251 val_loss:0.4538
[01/0079] | train_loss:0.3141 val_acc:80.0487 val_loss:0.4724
[01/0080] | train_loss:0.3213 val_acc:78.3455 val_loss:0.5155
[01/0081] | train_loss:0.3233 val_acc:81.2652 val_loss:0.4867
[01/0082] | train_loss:0.3225 val_acc:81.0219 val_loss:0.4539
[01/0083] | train_loss:0.2994 val_acc:81.5085 val_loss:0.4703
[01/0084] | train_loss:0.3004 val_acc:79.8054 val_loss:0.4855
[01/0085] | train_loss:0.2922 val_acc:79.8054 val_loss:0.475
[01/0086] | train_loss:0.3 val_acc:79.8054 val_loss:0.4749
[01/0087] | train_loss:0.286 val_acc:80.5353 val_loss:0.4998
[01/0088] | train_loss:0.2922 val_acc:80.5353 val_loss:0.4868
[01/0089] | train_loss:0.286 val_acc:78.3455 val_loss:0.5015
[01/0090] | train_loss:0.3058 val_acc:81.5085 val_loss:0.4501
[01/0091] | train_loss:0.2989 val_acc:78.8321 val_loss:0.5044
[01/0092] | train_loss:0.2938 val_acc:81.2652 val_loss:0.4602
[01/0093] | train_loss:0.301 val_acc:79.0754 val_loss:0.4974
[01/0094] | train_loss:0.2984 val_acc:79.3187 val_loss:0.4903
[01/0095] | train_loss:0.2766 val_acc:80.7786 val_loss:0.4868
[01/0096] | train_loss:0.2807 val_acc:80.292 val_loss:0.4779
[01/0097] | train_loss:0.2847 val_acc:80.5353 val_loss:0.4826
[01/0098] | train_loss:0.2795 val_acc:80.292 val_loss:0.4889
[01/0099] | train_loss:0.3034 val_acc:81.7518 val_loss:0.4599
[01/0100] | train_loss:0.2944 val_acc:81.0219 val_loss:0.4697
[01/0101] | train_loss:0.2769 val_acc:81.0219 val_loss:0.4743
[01/0102] | train_loss:0.2771 val_acc:81.7518 val_loss:0.4649
[01/0103] | train_loss:0.2782 val_acc:80.7786 val_loss:0.484
[01/0104] | train_loss:0.267 val_acc:80.5353 val_loss:0.4724
[01/0105] | train_loss:0.2759 val_acc:81.7518 val_loss:0.4699
[01/0106] | train_loss:0.2886 val_acc:81.2652 val_loss:0.484
Fold: [1/10] Test is finish !! 
 Test Metrics are: test_acc:85.4015 test_loss:0.3939fold [1/10] is start!!
[02/0001] | train_loss:0.6672 val_acc:54.0146 val_loss:0.6985
model is saved at epoch 1!![02/0002] | train_loss:0.6305 val_acc:54.2579 val_loss:0.7017
model is saved at epoch 2!![02/0003] | train_loss:0.607 val_acc:54.2579 val_loss:0.6907
[02/0004] | train_loss:0.585 val_acc:58.8808 val_loss:0.6698
model is saved at epoch 4!![02/0005] | train_loss:0.5679 val_acc:61.8005 val_loss:0.6443
model is saved at epoch 5!![02/0006] | train_loss:0.5569 val_acc:68.6131 val_loss:0.5787
model is saved at epoch 6!![02/0007] | train_loss:0.541 val_acc:74.4526 val_loss:0.5511
model is saved at epoch 7!![02/0008] | train_loss:0.5222 val_acc:75.4258 val_loss:0.5374
model is saved at epoch 8!![02/0009] | train_loss:0.507 val_acc:70.5596 val_loss:0.546
[02/0010] | train_loss:0.5286 val_acc:76.8856 val_loss:0.5258
model is saved at epoch 10!![02/0011] | train_loss:0.4938 val_acc:76.8856 val_loss:0.5134
[02/0012] | train_loss:0.4708 val_acc:78.5888 val_loss:0.4954
model is saved at epoch 12!![02/0013] | train_loss:0.4602 val_acc:78.3455 val_loss:0.4833
[02/0014] | train_loss:0.4516 val_acc:80.7786 val_loss:0.4669
model is saved at epoch 14!![02/0015] | train_loss:0.4443 val_acc:77.3723 val_loss:0.4991
[02/0016] | train_loss:0.442 val_acc:80.0487 val_loss:0.4763
[02/0017] | train_loss:0.4469 val_acc:79.562 val_loss:0.4618
[02/0018] | train_loss:0.438 val_acc:79.562 val_loss:0.4735
[02/0019] | train_loss:0.4301 val_acc:81.2652 val_loss:0.4433
model is saved at epoch 19!![02/0020] | train_loss:0.4112 val_acc:81.2652 val_loss:0.4384
[02/0021] | train_loss:0.4179 val_acc:81.0219 val_loss:0.4314
[02/0022] | train_loss:0.3998 val_acc:82.7251 val_loss:0.4293
model is saved at epoch 22!![02/0023] | train_loss:0.3882 val_acc:81.5085 val_loss:0.4245
[02/0024] | train_loss:0.3795 val_acc:84.4282 val_loss:0.4268
model is saved at epoch 24!![02/0025] | train_loss:0.3902 val_acc:80.5353 val_loss:0.431
[02/0026] | train_loss:0.3991 val_acc:82.2384 val_loss:0.4228
[02/0027] | train_loss:0.3681 val_acc:79.0754 val_loss:0.4521
[02/0028] | train_loss:0.3681 val_acc:83.6983 val_loss:0.4184
[02/0029] | train_loss:0.364 val_acc:80.292 val_loss:0.4294
[02/0030] | train_loss:0.3565 val_acc:83.2117 val_loss:0.4107
[02/0031] | train_loss:0.3562 val_acc:81.0219 val_loss:0.4189
[02/0032] | train_loss:0.3433 val_acc:83.9416 val_loss:0.4135
[02/0033] | train_loss:0.3326 val_acc:83.9416 val_loss:0.4076
[02/0034] | train_loss:0.336 val_acc:83.6983 val_loss:0.4171
[02/0035] | train_loss:0.3209 val_acc:82.7251 val_loss:0.4097
[02/0036] | train_loss:0.3175 val_acc:84.9148 val_loss:0.3994
model is saved at epoch 36!![02/0037] | train_loss:0.323 val_acc:83.6983 val_loss:0.4126
[02/0038] | train_loss:0.3112 val_acc:81.9951 val_loss:0.4072
[02/0039] | train_loss:0.3089 val_acc:84.1849 val_loss:0.4085
[02/0040] | train_loss:0.3009 val_acc:84.1849 val_loss:0.4118
[02/0041] | train_loss:0.3061 val_acc:81.0219 val_loss:0.4521
[02/0042] | train_loss:0.3034 val_acc:81.5085 val_loss:0.4513
[02/0043] | train_loss:0.2968 val_acc:84.9148 val_loss:0.4097
[02/0044] | train_loss:0.281 val_acc:84.1849 val_loss:0.4148
[02/0045] | train_loss:0.2838 val_acc:82.4818 val_loss:0.4197
[02/0046] | train_loss:0.3052 val_acc:84.1849 val_loss:0.4293
[02/0047] | train_loss:0.2853 val_acc:82.9684 val_loss:0.4299
[02/0048] | train_loss:0.2871 val_acc:83.2117 val_loss:0.4206
[02/0049] | train_loss:0.2647 val_acc:82.2384 val_loss:0.456
[02/0050] | train_loss:0.2681 val_acc:82.9684 val_loss:0.4313
[02/0051] | train_loss:0.2764 val_acc:85.8881 val_loss:0.4179
model is saved at epoch 51!![02/0052] | train_loss:0.2484 val_acc:84.6715 val_loss:0.4355
[02/0053] | train_loss:0.253 val_acc:84.6715 val_loss:0.4232
[02/0054] | train_loss:0.2458 val_acc:84.4282 val_loss:0.4563
[02/0055] | train_loss:0.2371 val_acc:83.9416 val_loss:0.4363
[02/0056] | train_loss:0.2462 val_acc:81.7518 val_loss:0.4534
[02/0057] | train_loss:0.2265 val_acc:84.1849 val_loss:0.4563
[02/0058] | train_loss:0.2235 val_acc:83.9416 val_loss:0.4584
[02/0059] | train_loss:0.2162 val_acc:82.7251 val_loss:0.437
[02/0060] | train_loss:0.2271 val_acc:82.4818 val_loss:0.4784
[02/0061] | train_loss:0.2097 val_acc:84.4282 val_loss:0.4626
[02/0062] | train_loss:0.2141 val_acc:82.7251 val_loss:0.4783
[02/0063] | train_loss:0.2219 val_acc:82.7251 val_loss:0.461
[02/0064] | train_loss:0.1905 val_acc:81.9951 val_loss:0.4894
[02/0065] | train_loss:0.1894 val_acc:83.2117 val_loss:0.4694
[02/0066] | train_loss:0.183 val_acc:82.9684 val_loss:0.4841
[02/0067] | train_loss:0.1882 val_acc:82.2384 val_loss:0.4654
[02/0068] | train_loss:0.1818 val_acc:84.4282 val_loss:0.4643
[02/0069] | train_loss:0.1774 val_acc:83.6983 val_loss:0.4617
[02/0070] | train_loss:0.1824 val_acc:80.0487 val_loss:0.5201
[02/0071] | train_loss:0.1792 val_acc:82.9684 val_loss:0.4902
[02/0072] | train_loss:0.1818 val_acc:82.9684 val_loss:0.4747
[02/0073] | train_loss:0.1987 val_acc:84.1849 val_loss:0.4838
[02/0074] | train_loss:0.1774 val_acc:83.2117 val_loss:0.49
[02/0075] | train_loss:0.1537 val_acc:82.7251 val_loss:0.4845
[02/0076] | train_loss:0.1563 val_acc:83.2117 val_loss:0.4877
[02/0077] | train_loss:0.15 val_acc:83.455 val_loss:0.4735
[02/0078] | train_loss:0.1588 val_acc:84.1849 val_loss:0.4837
[02/0079] | train_loss:0.1585 val_acc:82.4818 val_loss:0.5067
[02/0080] | train_loss:0.1587 val_acc:81.9951 val_loss:0.5023
[02/0081] | train_loss:0.1725 val_acc:78.5888 val_loss:0.5579
[02/0082] | train_loss:0.1705 val_acc:81.0219 val_loss:0.561
[02/0083] | train_loss:0.1751 val_acc:83.6983 val_loss:0.4982
[02/0084] | train_loss:0.1473 val_acc:82.9684 val_loss:0.5454
[02/0085] | train_loss:0.1475 val_acc:82.2384 val_loss:0.5267
[02/0086] | train_loss:0.1412 val_acc:83.455 val_loss:0.543
[02/0087] | train_loss:0.128 val_acc:85.1582 val_loss:0.5189
[02/0088] | train_loss:0.1299 val_acc:83.9416 val_loss:0.5099
[02/0089] | train_loss:0.1187 val_acc:82.2384 val_loss:0.5479
[02/0090] | train_loss:0.1416 val_acc:82.2384 val_loss:0.5522
[02/0091] | train_loss:0.1337 val_acc:83.9416 val_loss:0.5032
[02/0092] | train_loss:0.1234 val_acc:82.2384 val_loss:0.5628
[02/0093] | train_loss:0.1108 val_acc:82.9684 val_loss:0.5415
[02/0094] | train_loss:0.1036 val_acc:81.5085 val_loss:0.5828
[02/0095] | train_loss:0.1165 val_acc:82.9684 val_loss:0.5618
[02/0096] | train_loss:0.1147 val_acc:80.7786 val_loss:0.5913
[02/0097] | train_loss:0.1196 val_acc:82.2384 val_loss:0.5722
[02/0098] | train_loss:0.1139 val_acc:82.7251 val_loss:0.5317
[02/0099] | train_loss:0.1082 val_acc:81.5085 val_loss:0.5921
[02/0100] | train_loss:0.1204 val_acc:81.5085 val_loss:0.5968
[02/0101] | train_loss:0.1367 val_acc:82.7251 val_loss:0.547
[02/0102] | train_loss:0.1229 val_acc:84.1849 val_loss:0.5863
Fold: [2/10] Test is finish !! 
 Test Metrics are: test_acc:76.8856 test_loss:0.5594fold [2/10] is start!!
[03/0001] | train_loss:0.6649 val_acc:46.472 val_loss:0.7036
model is saved at epoch 1!![03/0002] | train_loss:0.6314 val_acc:46.472 val_loss:0.7258
[03/0003] | train_loss:0.6106 val_acc:58.1509 val_loss:0.683
model is saved at epoch 3!![03/0004] | train_loss:0.5829 val_acc:55.7178 val_loss:0.6726
[03/0005] | train_loss:0.5622 val_acc:61.8005 val_loss:0.64
model is saved at epoch 5!![03/0006] | train_loss:0.5419 val_acc:65.6934 val_loss:0.5971
model is saved at epoch 6!![03/0007] | train_loss:0.5282 val_acc:70.073 val_loss:0.5671
model is saved at epoch 7!![03/0008] | train_loss:0.5088 val_acc:71.2895 val_loss:0.5552
model is saved at epoch 8!![03/0009] | train_loss:0.4929 val_acc:74.4526 val_loss:0.5498
model is saved at epoch 9!![03/0010] | train_loss:0.4799 val_acc:73.236 val_loss:0.5362
[03/0011] | train_loss:0.4695 val_acc:75.9124 val_loss:0.54
model is saved at epoch 11!![03/0012] | train_loss:0.4547 val_acc:76.1557 val_loss:0.5461
model is saved at epoch 12!![03/0013] | train_loss:0.4553 val_acc:74.6959 val_loss:0.5209
[03/0014] | train_loss:0.4394 val_acc:75.1825 val_loss:0.523
[03/0015] | train_loss:0.4372 val_acc:73.9659 val_loss:0.5281
[03/0016] | train_loss:0.4304 val_acc:73.236 val_loss:0.5163
[03/0017] | train_loss:0.4226 val_acc:74.9392 val_loss:0.548
[03/0018] | train_loss:0.4 val_acc:74.2092 val_loss:0.5142
[03/0019] | train_loss:0.3936 val_acc:75.6691 val_loss:0.5115
[03/0020] | train_loss:0.3843 val_acc:77.129 val_loss:0.511
model is saved at epoch 20!![03/0021] | train_loss:0.3784 val_acc:76.6423 val_loss:0.5129
[03/0022] | train_loss:0.3771 val_acc:76.6423 val_loss:0.5079
[03/0023] | train_loss:0.3781 val_acc:78.3455 val_loss:0.5178
model is saved at epoch 23!![03/0024] | train_loss:0.3609 val_acc:78.3455 val_loss:0.4966
[03/0025] | train_loss:0.3619 val_acc:75.9124 val_loss:0.5063
[03/0026] | train_loss:0.3414 val_acc:77.129 val_loss:0.4936
[03/0027] | train_loss:0.3391 val_acc:78.8321 val_loss:0.5049
model is saved at epoch 27!![03/0028] | train_loss:0.327 val_acc:77.129 val_loss:0.5156
[03/0029] | train_loss:0.338 val_acc:76.1557 val_loss:0.5205
[03/0030] | train_loss:0.3159 val_acc:79.562 val_loss:0.5154
model is saved at epoch 30!![03/0031] | train_loss:0.3242 val_acc:78.5888 val_loss:0.5023
[03/0032] | train_loss:0.3075 val_acc:77.6156 val_loss:0.5144
[03/0033] | train_loss:0.3318 val_acc:78.1022 val_loss:0.5265
[03/0034] | train_loss:0.3075 val_acc:78.1022 val_loss:0.5081
[03/0035] | train_loss:0.2998 val_acc:77.129 val_loss:0.5053
[03/0036] | train_loss:0.3108 val_acc:74.6959 val_loss:0.5869
[03/0037] | train_loss:0.301 val_acc:78.5888 val_loss:0.4924
[03/0038] | train_loss:0.291 val_acc:78.3455 val_loss:0.5051
[03/0039] | train_loss:0.2756 val_acc:73.236 val_loss:0.6558
[03/0040] | train_loss:0.2938 val_acc:78.3455 val_loss:0.4964
[03/0041] | train_loss:0.2842 val_acc:76.6423 val_loss:0.5146
[03/0042] | train_loss:0.2654 val_acc:76.6423 val_loss:0.503
[03/0043] | train_loss:0.2542 val_acc:76.8856 val_loss:0.5745
[03/0044] | train_loss:0.2579 val_acc:79.3187 val_loss:0.5326
[03/0045] | train_loss:0.2616 val_acc:79.0754 val_loss:0.5064
[03/0046] | train_loss:0.2418 val_acc:79.562 val_loss:0.524
[03/0047] | train_loss:0.2323 val_acc:76.1557 val_loss:0.5789
[03/0048] | train_loss:0.2338 val_acc:77.8589 val_loss:0.5435
[03/0049] | train_loss:0.2247 val_acc:79.0754 val_loss:0.5252
[03/0050] | train_loss:0.2335 val_acc:77.129 val_loss:0.5578
[03/0051] | train_loss:0.2438 val_acc:79.0754 val_loss:0.5425
[03/0052] | train_loss:0.2307 val_acc:75.6691 val_loss:0.5728
[03/0053] | train_loss:0.2351 val_acc:79.3187 val_loss:0.5401
[03/0054] | train_loss:0.221 val_acc:79.0754 val_loss:0.5237
[03/0055] | train_loss:0.2104 val_acc:77.3723 val_loss:0.5507
[03/0056] | train_loss:0.2055 val_acc:79.0754 val_loss:0.5659
[03/0057] | train_loss:0.2015 val_acc:78.1022 val_loss:0.5571
[03/0058] | train_loss:0.1887 val_acc:79.3187 val_loss:0.5625
[03/0059] | train_loss:0.1762 val_acc:78.3455 val_loss:0.579
[03/0060] | train_loss:0.1969 val_acc:79.3187 val_loss:0.5907
[03/0061] | train_loss:0.1994 val_acc:78.5888 val_loss:0.5659
[03/0062] | train_loss:0.1864 val_acc:78.3455 val_loss:0.566
[03/0063] | train_loss:0.1806 val_acc:79.8054 val_loss:0.5515
model is saved at epoch 63!![03/0064] | train_loss:0.1739 val_acc:78.8321 val_loss:0.5826
[03/0065] | train_loss:0.1764 val_acc:79.8054 val_loss:0.6296
[03/0066] | train_loss:0.1797 val_acc:76.6423 val_loss:0.6123
[03/0067] | train_loss:0.1816 val_acc:77.8589 val_loss:0.5998
[03/0068] | train_loss:0.1792 val_acc:78.5888 val_loss:0.5868
[03/0069] | train_loss:0.1732 val_acc:78.8321 val_loss:0.6339
[03/0070] | train_loss:0.1616 val_acc:78.5888 val_loss:0.5556
[03/0071] | train_loss:0.1748 val_acc:78.1022 val_loss:0.5847
[03/0072] | train_loss:0.1654 val_acc:79.3187 val_loss:0.5846
[03/0073] | train_loss:0.1493 val_acc:80.7786 val_loss:0.6483
model is saved at epoch 73!![03/0074] | train_loss:0.1669 val_acc:78.1022 val_loss:0.597
[03/0075] | train_loss:0.1649 val_acc:79.3187 val_loss:0.6044
[03/0076] | train_loss:0.1355 val_acc:79.0754 val_loss:0.6266
[03/0077] | train_loss:0.1358 val_acc:79.0754 val_loss:0.6614
[03/0078] | train_loss:0.1677 val_acc:77.6156 val_loss:0.5896
[03/0079] | train_loss:0.1636 val_acc:78.3455 val_loss:0.6538
[03/0080] | train_loss:0.1597 val_acc:79.8054 val_loss:0.6269
[03/0081] | train_loss:0.1389 val_acc:77.3723 val_loss:0.6417
[03/0082] | train_loss:0.1419 val_acc:78.3455 val_loss:0.6263
[03/0083] | train_loss:0.1352 val_acc:78.3455 val_loss:0.6882
[03/0084] | train_loss:0.1483 val_acc:78.8321 val_loss:0.6293
[03/0085] | train_loss:0.1373 val_acc:79.0754 val_loss:0.6335
[03/0086] | train_loss:0.1412 val_acc:79.8054 val_loss:0.6843
[03/0087] | train_loss:0.1356 val_acc:78.8321 val_loss:0.6305
[03/0088] | train_loss:0.1177 val_acc:78.5888 val_loss:0.719
[03/0089] | train_loss:0.119 val_acc:78.8321 val_loss:0.6424
[03/0090] | train_loss:0.1109 val_acc:79.562 val_loss:0.6414
[03/0091] | train_loss:0.107 val_acc:78.8321 val_loss:0.6453
[03/0092] | train_loss:0.1089 val_acc:78.8321 val_loss:0.6885
[03/0093] | train_loss:0.119 val_acc:79.0754 val_loss:0.607
[03/0094] | train_loss:0.1181 val_acc:79.8054 val_loss:0.6495
[03/0095] | train_loss:0.0968 val_acc:80.5353 val_loss:0.6626
[03/0096] | train_loss:0.1043 val_acc:79.8054 val_loss:0.6617
[03/0097] | train_loss:0.1031 val_acc:80.0487 val_loss:0.6976
[03/0098] | train_loss:0.1033 val_acc:79.562 val_loss:0.6888
[03/0099] | train_loss:0.1007 val_acc:79.3187 val_loss:0.7383
[03/0100] | train_loss:0.1072 val_acc:80.7786 val_loss:0.6855
[03/0101] | train_loss:0.0904 val_acc:80.292 val_loss:0.7163
[03/0102] | train_loss:0.105 val_acc:78.8321 val_loss:0.7594
[03/0103] | train_loss:0.1024 val_acc:77.3723 val_loss:0.7733
[03/0104] | train_loss:0.0958 val_acc:79.562 val_loss:0.7352
[03/0105] | train_loss:0.0887 val_acc:79.0754 val_loss:0.7178
[03/0106] | train_loss:0.0998 val_acc:78.3455 val_loss:0.7798
[03/0107] | train_loss:0.093 val_acc:78.1022 val_loss:0.675
[03/0108] | train_loss:0.0942 val_acc:79.8054 val_loss:0.7536
[03/0109] | train_loss:0.0828 val_acc:78.8321 val_loss:0.7245
[03/0110] | train_loss:0.091 val_acc:79.562 val_loss:0.7324
[03/0111] | train_loss:0.0802 val_acc:81.0219 val_loss:0.725
model is saved at epoch 111!![03/0112] | train_loss:0.0836 val_acc:78.3455 val_loss:0.7235
[03/0113] | train_loss:0.0833 val_acc:80.292 val_loss:0.7539
[03/0114] | train_loss:0.0739 val_acc:79.3187 val_loss:0.8005
[03/0115] | train_loss:0.102 val_acc:79.8054 val_loss:0.713
[03/0116] | train_loss:0.0902 val_acc:80.5353 val_loss:0.7945
[03/0117] | train_loss:0.1213 val_acc:80.5353 val_loss:0.6641
[03/0118] | train_loss:0.0963 val_acc:79.0754 val_loss:0.7334
[03/0119] | train_loss:0.0658 val_acc:80.0487 val_loss:0.7831
[03/0120] | train_loss:0.0727 val_acc:80.5353 val_loss:0.7684
[03/0121] | train_loss:0.079 val_acc:79.562 val_loss:0.7617
[03/0122] | train_loss:0.0747 val_acc:80.292 val_loss:0.7852
[03/0123] | train_loss:0.0754 val_acc:80.5353 val_loss:0.7546
[03/0124] | train_loss:0.0653 val_acc:80.0487 val_loss:0.7869
[03/0125] | train_loss:0.0538 val_acc:80.0487 val_loss:0.8138
[03/0126] | train_loss:0.0605 val_acc:80.0487 val_loss:0.809
[03/0127] | train_loss:0.0584 val_acc:80.292 val_loss:0.7522
[03/0128] | train_loss:0.0668 val_acc:79.8054 val_loss:0.823
[03/0129] | train_loss:0.0673 val_acc:79.0754 val_loss:0.7977
[03/0130] | train_loss:0.0612 val_acc:80.7786 val_loss:0.8195
[03/0131] | train_loss:0.0622 val_acc:78.3455 val_loss:0.8785
[03/0132] | train_loss:0.0677 val_acc:80.0487 val_loss:0.8056
[03/0133] | train_loss:0.0718 val_acc:80.5353 val_loss:0.8911
[03/0134] | train_loss:0.0828 val_acc:78.3455 val_loss:0.798
[03/0135] | train_loss:0.0903 val_acc:78.5888 val_loss:0.7859
[03/0136] | train_loss:0.0713 val_acc:78.5888 val_loss:0.8469
[03/0137] | train_loss:0.0711 val_acc:77.8589 val_loss:0.8651
[03/0138] | train_loss:0.0855 val_acc:80.0487 val_loss:0.8282
[03/0139] | train_loss:0.069 val_acc:79.562 val_loss:0.8133
[03/0140] | train_loss:0.0642 val_acc:77.129 val_loss:0.9306
[03/0141] | train_loss:0.0588 val_acc:79.3187 val_loss:0.9094
[03/0142] | train_loss:0.0521 val_acc:80.5353 val_loss:0.8555
[03/0143] | train_loss:0.0596 val_acc:79.3187 val_loss:0.8432
[03/0144] | train_loss:0.0631 val_acc:81.0219 val_loss:0.8409
[03/0145] | train_loss:0.0533 val_acc:81.7518 val_loss:0.929
model is saved at epoch 145!![03/0146] | train_loss:0.0675 val_acc:79.0754 val_loss:0.8623
[03/0147] | train_loss:0.0637 val_acc:80.0487 val_loss:0.8557
[03/0148] | train_loss:0.0615 val_acc:79.0754 val_loss:0.8274
[03/0149] | train_loss:0.0623 val_acc:79.8054 val_loss:0.8082
[03/0150] | train_loss:0.0554 val_acc:79.3187 val_loss:0.8119
[03/0151] | train_loss:0.0484 val_acc:80.0487 val_loss:0.8672
[03/0152] | train_loss:0.0552 val_acc:80.0487 val_loss:0.8174
[03/0153] | train_loss:0.0547 val_acc:81.2652 val_loss:0.815
[03/0154] | train_loss:0.0485 val_acc:80.0487 val_loss:0.8404
[03/0155] | train_loss:0.0433 val_acc:80.0487 val_loss:0.9112
[03/0156] | train_loss:0.0515 val_acc:80.7786 val_loss:0.9741
[03/0157] | train_loss:0.0662 val_acc:80.0487 val_loss:0.8965
[03/0158] | train_loss:0.0657 val_acc:80.0487 val_loss:0.8723
[03/0159] | train_loss:0.0795 val_acc:80.292 val_loss:0.8083
[03/0160] | train_loss:0.0637 val_acc:80.0487 val_loss:0.8599
[03/0161] | train_loss:0.0573 val_acc:80.0487 val_loss:0.841
[03/0162] | train_loss:0.0657 val_acc:79.562 val_loss:0.9208
[03/0163] | train_loss:0.0555 val_acc:76.8856 val_loss:0.839
[03/0164] | train_loss:0.0575 val_acc:78.3455 val_loss:0.8588
[03/0165] | train_loss:0.0548 val_acc:79.3187 val_loss:0.8771
[03/0166] | train_loss:0.0493 val_acc:80.5353 val_loss:0.8807
[03/0167] | train_loss:0.053 val_acc:79.8054 val_loss:0.897
[03/0168] | train_loss:0.0494 val_acc:78.3455 val_loss:0.9036
[03/0169] | train_loss:0.0549 val_acc:80.0487 val_loss:0.8364
[03/0170] | train_loss:0.0568 val_acc:80.0487 val_loss:0.8593
[03/0171] | train_loss:0.0497 val_acc:79.0754 val_loss:0.9479
[03/0172] | train_loss:0.0505 val_acc:79.3187 val_loss:0.9604
[03/0173] | train_loss:0.0574 val_acc:79.8054 val_loss:0.8794
[03/0174] | train_loss:0.0575 val_acc:80.5353 val_loss:0.9531
[03/0175] | train_loss:0.0502 val_acc:81.7518 val_loss:0.8602
[03/0176] | train_loss:0.0445 val_acc:79.562 val_loss:0.8861
[03/0177] | train_loss:0.0679 val_acc:79.562 val_loss:0.9118
[03/0178] | train_loss:0.0614 val_acc:79.3187 val_loss:0.8788
[03/0179] | train_loss:0.0571 val_acc:80.7786 val_loss:0.9387
[03/0180] | train_loss:0.0442 val_acc:80.5353 val_loss:0.918
[03/0181] | train_loss:0.0463 val_acc:81.0219 val_loss:0.9346
[03/0182] | train_loss:0.0425 val_acc:80.5353 val_loss:0.8684
[03/0183] | train_loss:0.0396 val_acc:79.3187 val_loss:0.9574
[03/0184] | train_loss:0.0587 val_acc:77.8589 val_loss:0.9211
[03/0185] | train_loss:0.0541 val_acc:78.8321 val_loss:0.892
[03/0186] | train_loss:0.047 val_acc:80.292 val_loss:0.9834
[03/0187] | train_loss:0.0477 val_acc:77.8589 val_loss:0.9856
[03/0188] | train_loss:0.0393 val_acc:79.3187 val_loss:0.979
[03/0189] | train_loss:0.0432 val_acc:77.8589 val_loss:0.9271
[03/0190] | train_loss:0.0405 val_acc:81.2652 val_loss:0.9056
[03/0191] | train_loss:0.0435 val_acc:79.562 val_loss:1.0104
[03/0192] | train_loss:0.0553 val_acc:81.7518 val_loss:0.9042
[03/0193] | train_loss:0.0565 val_acc:81.5085 val_loss:0.9495
[03/0194] | train_loss:0.0558 val_acc:78.8321 val_loss:0.8805
[03/0195] | train_loss:0.0547 val_acc:81.0219 val_loss:0.8337
[03/0196] | train_loss:0.0406 val_acc:79.8054 val_loss:0.9184
Fold: [3/10] Test is finish !! 
 Test Metrics are: test_acc:81.2652 test_loss:0.8084fold [3/10] is start!!
[04/0001] | train_loss:0.6625 val_acc:50.6083 val_loss:0.6955
model is saved at epoch 1!![04/0002] | train_loss:0.6336 val_acc:48.6618 val_loss:0.6864
[04/0003] | train_loss:0.6086 val_acc:47.9319 val_loss:0.6955
[04/0004] | train_loss:0.5864 val_acc:53.0414 val_loss:0.682
model is saved at epoch 4!![04/0005] | train_loss:0.5664 val_acc:56.2044 val_loss:0.6529
model is saved at epoch 5!![04/0006] | train_loss:0.5501 val_acc:56.691 val_loss:0.6625
model is saved at epoch 6!![04/0007] | train_loss:0.5316 val_acc:68.3698 val_loss:0.5797
model is saved at epoch 7!![04/0008] | train_loss:0.5183 val_acc:73.4793 val_loss:0.5481
model is saved at epoch 8!![04/0009] | train_loss:0.499 val_acc:73.4793 val_loss:0.5417
[04/0010] | train_loss:0.4878 val_acc:73.4793 val_loss:0.5374
[04/0011] | train_loss:0.4729 val_acc:72.0195 val_loss:0.5393
[04/0012] | train_loss:0.4659 val_acc:73.9659 val_loss:0.554
model is saved at epoch 12!![04/0013] | train_loss:0.4517 val_acc:77.6156 val_loss:0.5192
model is saved at epoch 13!![04/0014] | train_loss:0.439 val_acc:71.2895 val_loss:0.5492
[04/0015] | train_loss:0.4378 val_acc:77.6156 val_loss:0.5117
[04/0016] | train_loss:0.4229 val_acc:74.6959 val_loss:0.5203
[04/0017] | train_loss:0.4117 val_acc:74.2092 val_loss:0.5229
[04/0018] | train_loss:0.42 val_acc:74.9392 val_loss:0.5126
[04/0019] | train_loss:0.3982 val_acc:77.8589 val_loss:0.5167
model is saved at epoch 19!![04/0020] | train_loss:0.3893 val_acc:75.6691 val_loss:0.5273
[04/0021] | train_loss:0.3791 val_acc:76.1557 val_loss:0.4991
[04/0022] | train_loss:0.3798 val_acc:74.9392 val_loss:0.5075
[04/0023] | train_loss:0.3585 val_acc:75.6691 val_loss:0.5085
[04/0024] | train_loss:0.3569 val_acc:79.562 val_loss:0.4851
model is saved at epoch 24!![04/0025] | train_loss:0.3526 val_acc:77.8589 val_loss:0.5038
[04/0026] | train_loss:0.3435 val_acc:77.129 val_loss:0.5368
[04/0027] | train_loss:0.3492 val_acc:78.8321 val_loss:0.4975
[04/0028] | train_loss:0.3339 val_acc:77.6156 val_loss:0.5081
[04/0029] | train_loss:0.3229 val_acc:77.129 val_loss:0.5163
[04/0030] | train_loss:0.3233 val_acc:79.0754 val_loss:0.4947
[04/0031] | train_loss:0.3143 val_acc:77.129 val_loss:0.5206
[04/0032] | train_loss:0.3183 val_acc:80.0487 val_loss:0.4966
model is saved at epoch 32!![04/0033] | train_loss:0.3034 val_acc:76.8856 val_loss:0.5433
[04/0034] | train_loss:0.3076 val_acc:78.3455 val_loss:0.5179
[04/0035] | train_loss:0.2995 val_acc:81.0219 val_loss:0.5085
model is saved at epoch 35!![04/0036] | train_loss:0.2798 val_acc:77.6156 val_loss:0.5276
[04/0037] | train_loss:0.2798 val_acc:79.8054 val_loss:0.5231
[04/0038] | train_loss:0.2697 val_acc:79.8054 val_loss:0.5214
[04/0039] | train_loss:0.2689 val_acc:78.3455 val_loss:0.5435
[04/0040] | train_loss:0.2627 val_acc:80.0487 val_loss:0.5414
[04/0041] | train_loss:0.2547 val_acc:78.3455 val_loss:0.5642
[04/0042] | train_loss:0.2469 val_acc:78.3455 val_loss:0.5539
[04/0043] | train_loss:0.2581 val_acc:80.292 val_loss:0.5264
[04/0044] | train_loss:0.2352 val_acc:78.8321 val_loss:0.5734
[04/0045] | train_loss:0.2422 val_acc:79.8054 val_loss:0.5677
[04/0046] | train_loss:0.228 val_acc:81.0219 val_loss:0.5446
[04/0047] | train_loss:0.2416 val_acc:78.3455 val_loss:0.5635
[04/0048] | train_loss:0.2342 val_acc:78.5888 val_loss:0.5608
[04/0049] | train_loss:0.2178 val_acc:78.3455 val_loss:0.5781
[04/0050] | train_loss:0.2131 val_acc:79.562 val_loss:0.6027
[04/0051] | train_loss:0.2112 val_acc:78.8321 val_loss:0.5664
[04/0052] | train_loss:0.196 val_acc:79.562 val_loss:0.5677
[04/0053] | train_loss:0.1921 val_acc:81.5085 val_loss:0.5752
model is saved at epoch 53!![04/0054] | train_loss:0.1992 val_acc:79.562 val_loss:0.5776
[04/0055] | train_loss:0.1939 val_acc:79.0754 val_loss:0.6046
[04/0056] | train_loss:0.1912 val_acc:80.5353 val_loss:0.5944
[04/0057] | train_loss:0.1888 val_acc:78.8321 val_loss:0.6053
[04/0058] | train_loss:0.1701 val_acc:80.292 val_loss:0.6237
[04/0059] | train_loss:0.1606 val_acc:80.0487 val_loss:0.6176
[04/0060] | train_loss:0.169 val_acc:79.8054 val_loss:0.5966
[04/0061] | train_loss:0.1588 val_acc:78.5888 val_loss:0.6257
[04/0062] | train_loss:0.1666 val_acc:78.5888 val_loss:0.5998
[04/0063] | train_loss:0.1636 val_acc:80.292 val_loss:0.5965
[04/0064] | train_loss:0.1604 val_acc:81.2652 val_loss:0.5812
[04/0065] | train_loss:0.1382 val_acc:80.7786 val_loss:0.6445
[04/0066] | train_loss:0.1343 val_acc:78.8321 val_loss:0.6461
[04/0067] | train_loss:0.1505 val_acc:78.8321 val_loss:0.6442
[04/0068] | train_loss:0.141 val_acc:80.292 val_loss:0.6255
[04/0069] | train_loss:0.1199 val_acc:80.0487 val_loss:0.7244
[04/0070] | train_loss:0.1381 val_acc:80.0487 val_loss:0.6244
[04/0071] | train_loss:0.1481 val_acc:80.5353 val_loss:0.6522
[04/0072] | train_loss:0.1306 val_acc:80.7786 val_loss:0.6677
[04/0073] | train_loss:0.1227 val_acc:78.5888 val_loss:0.6701
[04/0074] | train_loss:0.1408 val_acc:81.7518 val_loss:0.662
model is saved at epoch 74!![04/0075] | train_loss:0.1329 val_acc:79.562 val_loss:0.6457
[04/0076] | train_loss:0.1214 val_acc:79.8054 val_loss:0.6469
[04/0077] | train_loss:0.1194 val_acc:79.0754 val_loss:0.6913
[04/0078] | train_loss:0.1243 val_acc:79.8054 val_loss:0.6993
[04/0079] | train_loss:0.1161 val_acc:81.0219 val_loss:0.6596
[04/0080] | train_loss:0.1223 val_acc:79.562 val_loss:0.7427
[04/0081] | train_loss:0.1384 val_acc:77.3723 val_loss:0.6978
[04/0082] | train_loss:0.1589 val_acc:80.7786 val_loss:0.6226
[04/0083] | train_loss:0.1189 val_acc:79.562 val_loss:0.691
[04/0084] | train_loss:0.1037 val_acc:80.7786 val_loss:0.7274
[04/0085] | train_loss:0.1089 val_acc:80.5353 val_loss:0.7435
[04/0086] | train_loss:0.1057 val_acc:79.8054 val_loss:0.7185
[04/0087] | train_loss:0.1009 val_acc:80.5353 val_loss:0.7316
[04/0088] | train_loss:0.1016 val_acc:78.3455 val_loss:0.7528
[04/0089] | train_loss:0.086 val_acc:80.5353 val_loss:0.7295
[04/0090] | train_loss:0.0885 val_acc:80.292 val_loss:0.7827
[04/0091] | train_loss:0.1005 val_acc:79.8054 val_loss:0.7669
[04/0092] | train_loss:0.0901 val_acc:79.8054 val_loss:0.7389
[04/0093] | train_loss:0.0857 val_acc:79.562 val_loss:0.7685
[04/0094] | train_loss:0.084 val_acc:80.0487 val_loss:0.7391
[04/0095] | train_loss:0.0954 val_acc:79.3187 val_loss:0.7696
[04/0096] | train_loss:0.0986 val_acc:77.8589 val_loss:0.7667
[04/0097] | train_loss:0.0857 val_acc:78.1022 val_loss:0.8241
[04/0098] | train_loss:0.0838 val_acc:80.292 val_loss:0.7631
[04/0099] | train_loss:0.0766 val_acc:79.8054 val_loss:0.8086
[04/0100] | train_loss:0.0861 val_acc:80.0487 val_loss:0.7952
[04/0101] | train_loss:0.0796 val_acc:79.8054 val_loss:0.797
[04/0102] | train_loss:0.0676 val_acc:79.3187 val_loss:0.8414
[04/0103] | train_loss:0.073 val_acc:79.3187 val_loss:0.8103
[04/0104] | train_loss:0.0795 val_acc:80.0487 val_loss:0.8112
[04/0105] | train_loss:0.0737 val_acc:79.562 val_loss:0.8112
[04/0106] | train_loss:0.077 val_acc:80.292 val_loss:0.8507
[04/0107] | train_loss:0.0667 val_acc:80.7786 val_loss:0.8164
[04/0108] | train_loss:0.0828 val_acc:79.562 val_loss:0.8032
[04/0109] | train_loss:0.0799 val_acc:78.1022 val_loss:0.8588
[04/0110] | train_loss:0.0769 val_acc:80.0487 val_loss:0.8086
[04/0111] | train_loss:0.0729 val_acc:78.3455 val_loss:0.8595
[04/0112] | train_loss:0.0813 val_acc:79.562 val_loss:0.8319
[04/0113] | train_loss:0.0782 val_acc:80.5353 val_loss:0.835
[04/0114] | train_loss:0.0803 val_acc:77.8589 val_loss:0.8314
[04/0115] | train_loss:0.0843 val_acc:79.8054 val_loss:0.8799
[04/0116] | train_loss:0.0966 val_acc:78.3455 val_loss:0.8164
[04/0117] | train_loss:0.0749 val_acc:78.1022 val_loss:0.8846
[04/0118] | train_loss:0.071 val_acc:79.562 val_loss:0.8869
[04/0119] | train_loss:0.0745 val_acc:76.8856 val_loss:0.9114
[04/0120] | train_loss:0.0615 val_acc:79.8054 val_loss:0.8823
[04/0121] | train_loss:0.0605 val_acc:80.0487 val_loss:0.9113
[04/0122] | train_loss:0.0546 val_acc:79.562 val_loss:0.9531
[04/0123] | train_loss:0.0688 val_acc:80.0487 val_loss:0.9483
[04/0124] | train_loss:0.0689 val_acc:78.1022 val_loss:0.9033
[04/0125] | train_loss:0.0648 val_acc:78.8321 val_loss:0.9259
Fold: [4/10] Test is finish !! 
 Test Metrics are: test_acc:81.0219 test_loss:0.6777fold [4/10] is start!!
[05/0001] | train_loss:0.6685 val_acc:50.6083 val_loss:0.7
model is saved at epoch 1!![05/0002] | train_loss:0.632 val_acc:51.3382 val_loss:0.6893
model is saved at epoch 2!![05/0003] | train_loss:0.611 val_acc:48.9051 val_loss:0.6916
[05/0004] | train_loss:0.596 val_acc:50.365 val_loss:0.6814
[05/0005] | train_loss:0.5771 val_acc:59.3674 val_loss:0.6312
model is saved at epoch 5!![05/0006] | train_loss:0.559 val_acc:70.5596 val_loss:0.5813
model is saved at epoch 6!![05/0007] | train_loss:0.536 val_acc:68.8564 val_loss:0.563
[05/0008] | train_loss:0.5314 val_acc:73.4793 val_loss:0.5433
model is saved at epoch 8!![05/0009] | train_loss:0.5021 val_acc:74.2092 val_loss:0.5353
model is saved at epoch 9!![05/0010] | train_loss:0.4874 val_acc:75.6691 val_loss:0.5218
model is saved at epoch 10!![05/0011] | train_loss:0.4767 val_acc:74.6959 val_loss:0.5171
[05/0012] | train_loss:0.4656 val_acc:77.6156 val_loss:0.5137
model is saved at epoch 12!![05/0013] | train_loss:0.4515 val_acc:76.8856 val_loss:0.5316
[05/0014] | train_loss:0.4319 val_acc:77.3723 val_loss:0.513
[05/0015] | train_loss:0.4181 val_acc:76.399 val_loss:0.5133
[05/0016] | train_loss:0.4096 val_acc:77.8589 val_loss:0.5009
model is saved at epoch 16!![05/0017] | train_loss:0.4017 val_acc:79.3187 val_loss:0.5079
model is saved at epoch 17!![05/0018] | train_loss:0.39 val_acc:76.1557 val_loss:0.5232
[05/0019] | train_loss:0.3728 val_acc:79.0754 val_loss:0.4998
[05/0020] | train_loss:0.3711 val_acc:76.6423 val_loss:0.4988
[05/0021] | train_loss:0.3596 val_acc:78.1022 val_loss:0.4964
[05/0022] | train_loss:0.3654 val_acc:76.399 val_loss:0.4981
[05/0023] | train_loss:0.3447 val_acc:78.3455 val_loss:0.4944
[05/0024] | train_loss:0.342 val_acc:78.1022 val_loss:0.4902
[05/0025] | train_loss:0.3318 val_acc:77.6156 val_loss:0.5111
[05/0026] | train_loss:0.3159 val_acc:77.8589 val_loss:0.496
[05/0027] | train_loss:0.3082 val_acc:78.3455 val_loss:0.5285
[05/0028] | train_loss:0.3001 val_acc:79.0754 val_loss:0.4911
[05/0029] | train_loss:0.2992 val_acc:80.5353 val_loss:0.4987
model is saved at epoch 29!![05/0030] | train_loss:0.2943 val_acc:80.7786 val_loss:0.4812
model is saved at epoch 30!![05/0031] | train_loss:0.2806 val_acc:80.5353 val_loss:0.4746
[05/0032] | train_loss:0.2764 val_acc:76.8856 val_loss:0.4947
[05/0033] | train_loss:0.2764 val_acc:78.5888 val_loss:0.5002
[05/0034] | train_loss:0.2648 val_acc:79.0754 val_loss:0.4933
[05/0035] | train_loss:0.2571 val_acc:79.0754 val_loss:0.563
[05/0036] | train_loss:0.2712 val_acc:80.5353 val_loss:0.5161
[05/0037] | train_loss:0.2449 val_acc:78.8321 val_loss:0.5152
[05/0038] | train_loss:0.2511 val_acc:79.3187 val_loss:0.4988
[05/0039] | train_loss:0.2372 val_acc:77.8589 val_loss:0.583
[05/0040] | train_loss:0.2357 val_acc:78.1022 val_loss:0.5232
[05/0041] | train_loss:0.2225 val_acc:78.5888 val_loss:0.4884
[05/0042] | train_loss:0.2011 val_acc:81.0219 val_loss:0.5051
model is saved at epoch 42!![05/0043] | train_loss:0.1975 val_acc:81.0219 val_loss:0.5071
[05/0044] | train_loss:0.1921 val_acc:80.0487 val_loss:0.5225
[05/0045] | train_loss:0.2188 val_acc:80.292 val_loss:0.4865
[05/0046] | train_loss:0.2055 val_acc:80.7786 val_loss:0.4867
[05/0047] | train_loss:0.199 val_acc:81.2652 val_loss:0.4918
model is saved at epoch 47!![05/0048] | train_loss:0.1878 val_acc:78.8321 val_loss:0.5365
[05/0049] | train_loss:0.1803 val_acc:77.3723 val_loss:0.6005
[05/0050] | train_loss:0.181 val_acc:79.8054 val_loss:0.5049
[05/0051] | train_loss:0.1928 val_acc:81.9951 val_loss:0.5388
model is saved at epoch 51!![05/0052] | train_loss:0.1842 val_acc:79.3187 val_loss:0.5945
[05/0053] | train_loss:0.1596 val_acc:81.7518 val_loss:0.5393
[05/0054] | train_loss:0.169 val_acc:78.5888 val_loss:0.5752
[05/0055] | train_loss:0.193 val_acc:81.5085 val_loss:0.5073
[05/0056] | train_loss:0.1577 val_acc:80.0487 val_loss:0.5795
[05/0057] | train_loss:0.1535 val_acc:81.5085 val_loss:0.5075
[05/0058] | train_loss:0.157 val_acc:79.8054 val_loss:0.5545
[05/0059] | train_loss:0.1353 val_acc:81.9951 val_loss:0.5476
[05/0060] | train_loss:0.1253 val_acc:81.0219 val_loss:0.604
[05/0061] | train_loss:0.1329 val_acc:82.4818 val_loss:0.5579
model is saved at epoch 61!![05/0062] | train_loss:0.1291 val_acc:81.7518 val_loss:0.5438
[05/0063] | train_loss:0.1304 val_acc:82.7251 val_loss:0.5801
model is saved at epoch 63!![05/0064] | train_loss:0.1311 val_acc:81.7518 val_loss:0.5517
[05/0065] | train_loss:0.1159 val_acc:82.7251 val_loss:0.5764
[05/0066] | train_loss:0.1067 val_acc:81.2652 val_loss:0.5797
[05/0067] | train_loss:0.1233 val_acc:82.9684 val_loss:0.5504
model is saved at epoch 67!![05/0068] | train_loss:0.1073 val_acc:79.8054 val_loss:0.6404
[05/0069] | train_loss:0.1019 val_acc:80.5353 val_loss:0.583
[05/0070] | train_loss:0.1045 val_acc:83.9416 val_loss:0.5568
model is saved at epoch 70!![05/0071] | train_loss:0.1034 val_acc:80.7786 val_loss:0.5948
[05/0072] | train_loss:0.1019 val_acc:81.9951 val_loss:0.5559
[05/0073] | train_loss:0.1089 val_acc:81.9951 val_loss:0.5866
[05/0074] | train_loss:0.1037 val_acc:83.2117 val_loss:0.5681
[05/0075] | train_loss:0.1092 val_acc:81.2652 val_loss:0.5571
[05/0076] | train_loss:0.1049 val_acc:83.9416 val_loss:0.6519
[05/0077] | train_loss:0.0964 val_acc:80.7786 val_loss:0.6509
[05/0078] | train_loss:0.0913 val_acc:79.3187 val_loss:0.6293
[05/0079] | train_loss:0.0948 val_acc:81.5085 val_loss:0.6168
[05/0080] | train_loss:0.0906 val_acc:81.0219 val_loss:0.6214
[05/0081] | train_loss:0.0934 val_acc:80.7786 val_loss:0.6584
[05/0082] | train_loss:0.0768 val_acc:81.7518 val_loss:0.6525
[05/0083] | train_loss:0.0771 val_acc:81.5085 val_loss:0.6671
[05/0084] | train_loss:0.0794 val_acc:81.2652 val_loss:0.6994
[05/0085] | train_loss:0.0889 val_acc:80.5353 val_loss:0.6401
[05/0086] | train_loss:0.0971 val_acc:83.2117 val_loss:0.6369
[05/0087] | train_loss:0.0779 val_acc:81.2652 val_loss:0.7044
[05/0088] | train_loss:0.0966 val_acc:80.7786 val_loss:0.6839
[05/0089] | train_loss:0.0835 val_acc:80.5353 val_loss:0.7155
[05/0090] | train_loss:0.0756 val_acc:80.292 val_loss:0.6888
[05/0091] | train_loss:0.0812 val_acc:82.4818 val_loss:0.7014
[05/0092] | train_loss:0.0847 val_acc:81.9951 val_loss:0.5917
[05/0093] | train_loss:0.085 val_acc:82.7251 val_loss:0.6942
[05/0094] | train_loss:0.0799 val_acc:80.5353 val_loss:0.7124
[05/0095] | train_loss:0.0661 val_acc:81.5085 val_loss:0.6794
[05/0096] | train_loss:0.0669 val_acc:81.5085 val_loss:0.6519
[05/0097] | train_loss:0.0698 val_acc:81.5085 val_loss:0.6965
[05/0098] | train_loss:0.0603 val_acc:82.4818 val_loss:0.676
[05/0099] | train_loss:0.0637 val_acc:80.292 val_loss:0.7084
[05/0100] | train_loss:0.0561 val_acc:80.292 val_loss:0.6892
[05/0101] | train_loss:0.0589 val_acc:82.2384 val_loss:0.7029
[05/0102] | train_loss:0.0526 val_acc:81.7518 val_loss:0.7327
[05/0103] | train_loss:0.0596 val_acc:81.2652 val_loss:0.7227
[05/0104] | train_loss:0.0776 val_acc:80.5353 val_loss:0.7026
[05/0105] | train_loss:0.0617 val_acc:81.7518 val_loss:0.7136
[05/0106] | train_loss:0.0539 val_acc:80.7786 val_loss:0.7346
[05/0107] | train_loss:0.0452 val_acc:82.9684 val_loss:0.6771
[05/0108] | train_loss:0.0519 val_acc:81.7518 val_loss:0.7392
[05/0109] | train_loss:0.0599 val_acc:81.9951 val_loss:0.72
[05/0110] | train_loss:0.0506 val_acc:82.7251 val_loss:0.709
[05/0111] | train_loss:0.0514 val_acc:79.8054 val_loss:0.7738
[05/0112] | train_loss:0.0655 val_acc:82.2384 val_loss:0.7485
[05/0113] | train_loss:0.0622 val_acc:78.1022 val_loss:0.82
[05/0114] | train_loss:0.0536 val_acc:80.292 val_loss:0.7787
[05/0115] | train_loss:0.0365 val_acc:82.7251 val_loss:0.8091
[05/0116] | train_loss:0.0538 val_acc:81.0219 val_loss:0.7968
[05/0117] | train_loss:0.065 val_acc:82.4818 val_loss:0.7432
[05/0118] | train_loss:0.0549 val_acc:83.455 val_loss:0.7599
[05/0119] | train_loss:0.0448 val_acc:78.1022 val_loss:0.8659
[05/0120] | train_loss:0.071 val_acc:82.7251 val_loss:0.7355
[05/0121] | train_loss:0.0626 val_acc:81.5085 val_loss:0.7602
Fold: [5/10] Test is finish !! 
 Test Metrics are: test_acc:84.4282 test_loss:0.5375fold [5/10] is start!!
[06/0001] | train_loss:0.6704 val_acc:48.9051 val_loss:0.6946
model is saved at epoch 1!![06/0002] | train_loss:0.6225 val_acc:55.7178 val_loss:0.6891
model is saved at epoch 2!![06/0003] | train_loss:0.5984 val_acc:63.5036 val_loss:0.6807
model is saved at epoch 3!![06/0004] | train_loss:0.5736 val_acc:63.017 val_loss:0.6718
[06/0005] | train_loss:0.5557 val_acc:59.6107 val_loss:0.6189
[06/0006] | train_loss:0.5391 val_acc:70.8029 val_loss:0.5512
model is saved at epoch 6!![06/0007] | train_loss:0.5263 val_acc:71.5328 val_loss:0.5349
model is saved at epoch 7!![06/0008] | train_loss:0.5173 val_acc:75.1825 val_loss:0.5098
model is saved at epoch 8!![06/0009] | train_loss:0.5042 val_acc:72.7494 val_loss:0.5514
[06/0010] | train_loss:0.4993 val_acc:74.4526 val_loss:0.5246
[06/0011] | train_loss:0.4808 val_acc:76.1557 val_loss:0.4821
model is saved at epoch 11!![06/0012] | train_loss:0.4559 val_acc:78.5888 val_loss:0.4741
model is saved at epoch 12!![06/0013] | train_loss:0.4439 val_acc:78.5888 val_loss:0.4559
[06/0014] | train_loss:0.4381 val_acc:75.6691 val_loss:0.5293
[06/0015] | train_loss:0.426 val_acc:79.0754 val_loss:0.4616
model is saved at epoch 15!![06/0016] | train_loss:0.4188 val_acc:79.3187 val_loss:0.4589
model is saved at epoch 16!![06/0017] | train_loss:0.3962 val_acc:80.0487 val_loss:0.4597
model is saved at epoch 17!![06/0018] | train_loss:0.3844 val_acc:81.2652 val_loss:0.4566
model is saved at epoch 18!![06/0019] | train_loss:0.3758 val_acc:79.8054 val_loss:0.4521
[06/0020] | train_loss:0.3736 val_acc:78.8321 val_loss:0.4545
[06/0021] | train_loss:0.3661 val_acc:80.0487 val_loss:0.4422
[06/0022] | train_loss:0.3736 val_acc:78.8321 val_loss:0.4535
[06/0023] | train_loss:0.3304 val_acc:80.7786 val_loss:0.4519
[06/0024] | train_loss:0.3282 val_acc:79.0754 val_loss:0.4435
[06/0025] | train_loss:0.3177 val_acc:80.5353 val_loss:0.4417
[06/0026] | train_loss:0.3131 val_acc:79.3187 val_loss:0.4552
[06/0027] | train_loss:0.2999 val_acc:80.0487 val_loss:0.482
[06/0028] | train_loss:0.295 val_acc:82.7251 val_loss:0.4301
model is saved at epoch 28!![06/0029] | train_loss:0.2783 val_acc:79.3187 val_loss:0.4718
[06/0030] | train_loss:0.273 val_acc:81.9951 val_loss:0.4387
[06/0031] | train_loss:0.2614 val_acc:81.9951 val_loss:0.4382
[06/0032] | train_loss:0.2525 val_acc:81.0219 val_loss:0.4665
[06/0033] | train_loss:0.2694 val_acc:80.0487 val_loss:0.4761
[06/0034] | train_loss:0.2328 val_acc:78.3455 val_loss:0.4674
[06/0035] | train_loss:0.2202 val_acc:80.7786 val_loss:0.4951
[06/0036] | train_loss:0.2214 val_acc:80.5353 val_loss:0.4951
[06/0037] | train_loss:0.2141 val_acc:81.2652 val_loss:0.507
[06/0038] | train_loss:0.2205 val_acc:80.5353 val_loss:0.4756
[06/0039] | train_loss:0.2089 val_acc:80.0487 val_loss:0.4716
[06/0040] | train_loss:0.1959 val_acc:80.5353 val_loss:0.4949
[06/0041] | train_loss:0.1884 val_acc:80.292 val_loss:0.478
[06/0042] | train_loss:0.173 val_acc:79.562 val_loss:0.4942
[06/0043] | train_loss:0.1743 val_acc:83.2117 val_loss:0.4804
model is saved at epoch 43!![06/0044] | train_loss:0.1676 val_acc:80.7786 val_loss:0.4964
[06/0045] | train_loss:0.1668 val_acc:81.0219 val_loss:0.5779
[06/0046] | train_loss:0.1742 val_acc:80.5353 val_loss:0.4901
[06/0047] | train_loss:0.1908 val_acc:79.0754 val_loss:0.5337
[06/0048] | train_loss:0.1757 val_acc:82.2384 val_loss:0.5114
[06/0049] | train_loss:0.1585 val_acc:81.9951 val_loss:0.4969
[06/0050] | train_loss:0.1497 val_acc:80.7786 val_loss:0.514
[06/0051] | train_loss:0.1507 val_acc:82.2384 val_loss:0.5267
[06/0052] | train_loss:0.1342 val_acc:81.2652 val_loss:0.5113
[06/0053] | train_loss:0.1411 val_acc:81.7518 val_loss:0.5027
[06/0054] | train_loss:0.1195 val_acc:81.7518 val_loss:0.5707
[06/0055] | train_loss:0.1294 val_acc:81.5085 val_loss:0.5738
[06/0056] | train_loss:0.1293 val_acc:79.0754 val_loss:0.5345
[06/0057] | train_loss:0.1188 val_acc:80.5353 val_loss:0.5525
[06/0058] | train_loss:0.1222 val_acc:82.7251 val_loss:0.5254
[06/0059] | train_loss:0.1109 val_acc:81.9951 val_loss:0.5671
[06/0060] | train_loss:0.1114 val_acc:81.0219 val_loss:0.5804
[06/0061] | train_loss:0.1174 val_acc:80.7786 val_loss:0.5669
[06/0062] | train_loss:0.1323 val_acc:82.9684 val_loss:0.5459
[06/0063] | train_loss:0.0898 val_acc:82.4818 val_loss:0.5616
[06/0064] | train_loss:0.1161 val_acc:81.7518 val_loss:0.576
[06/0065] | train_loss:0.0996 val_acc:82.2384 val_loss:0.5632
[06/0066] | train_loss:0.094 val_acc:82.2384 val_loss:0.5876
[06/0067] | train_loss:0.088 val_acc:81.9951 val_loss:0.5875
[06/0068] | train_loss:0.0707 val_acc:82.2384 val_loss:0.5807
[06/0069] | train_loss:0.0806 val_acc:82.4818 val_loss:0.563
[06/0070] | train_loss:0.1077 val_acc:80.5353 val_loss:0.6474
[06/0071] | train_loss:0.0804 val_acc:81.0219 val_loss:0.5622
[06/0072] | train_loss:0.0795 val_acc:82.7251 val_loss:0.5702
[06/0073] | train_loss:0.0802 val_acc:82.9684 val_loss:0.5865
[06/0074] | train_loss:0.0711 val_acc:81.9951 val_loss:0.6047
[06/0075] | train_loss:0.07 val_acc:83.9416 val_loss:0.6157
model is saved at epoch 75!![06/0076] | train_loss:0.0729 val_acc:80.5353 val_loss:0.6382
[06/0077] | train_loss:0.0762 val_acc:83.2117 val_loss:0.5899
[06/0078] | train_loss:0.0828 val_acc:82.7251 val_loss:0.6163
[06/0079] | train_loss:0.074 val_acc:81.7518 val_loss:0.6432
[06/0080] | train_loss:0.068 val_acc:82.2384 val_loss:0.6595
[06/0081] | train_loss:0.0622 val_acc:83.2117 val_loss:0.6262
[06/0082] | train_loss:0.0627 val_acc:82.9684 val_loss:0.6125
[06/0083] | train_loss:0.0598 val_acc:82.9684 val_loss:0.6602
[06/0084] | train_loss:0.0622 val_acc:81.0219 val_loss:0.6515
[06/0085] | train_loss:0.0614 val_acc:81.9951 val_loss:0.7468
[06/0086] | train_loss:0.0626 val_acc:82.9684 val_loss:0.6096
[06/0087] | train_loss:0.0559 val_acc:81.9951 val_loss:0.7268
[06/0088] | train_loss:0.062 val_acc:83.2117 val_loss:0.6446
[06/0089] | train_loss:0.0562 val_acc:81.2652 val_loss:0.6682
[06/0090] | train_loss:0.0654 val_acc:81.9951 val_loss:0.6359
[06/0091] | train_loss:0.0541 val_acc:82.7251 val_loss:0.6974
[06/0092] | train_loss:0.0546 val_acc:81.7518 val_loss:0.652
[06/0093] | train_loss:0.0549 val_acc:83.6983 val_loss:0.6556
[06/0094] | train_loss:0.0578 val_acc:81.5085 val_loss:0.674
[06/0095] | train_loss:0.0536 val_acc:82.4818 val_loss:0.6846
[06/0096] | train_loss:0.0545 val_acc:81.2652 val_loss:0.7165
[06/0097] | train_loss:0.0481 val_acc:82.4818 val_loss:0.7108
[06/0098] | train_loss:0.0702 val_acc:80.7786 val_loss:0.6234
[06/0099] | train_loss:0.0676 val_acc:81.9951 val_loss:0.6992
[06/0100] | train_loss:0.0586 val_acc:82.2384 val_loss:0.7002
[06/0101] | train_loss:0.0535 val_acc:80.292 val_loss:0.7307
[06/0102] | train_loss:0.0731 val_acc:81.2652 val_loss:0.735
[06/0103] | train_loss:0.0472 val_acc:82.7251 val_loss:0.6846
[06/0104] | train_loss:0.0502 val_acc:81.7518 val_loss:0.7713
[06/0105] | train_loss:0.0484 val_acc:81.5085 val_loss:0.6901
[06/0106] | train_loss:0.0534 val_acc:80.5353 val_loss:0.7692
[06/0107] | train_loss:0.0487 val_acc:82.2384 val_loss:0.695
[06/0108] | train_loss:0.047 val_acc:82.4818 val_loss:0.7752
[06/0109] | train_loss:0.0484 val_acc:83.455 val_loss:0.6809
[06/0110] | train_loss:0.0422 val_acc:81.0219 val_loss:0.7797
[06/0111] | train_loss:0.049 val_acc:83.2117 val_loss:0.7318
[06/0112] | train_loss:0.0423 val_acc:80.5353 val_loss:0.7691
[06/0113] | train_loss:0.0382 val_acc:83.2117 val_loss:0.7338
[06/0114] | train_loss:0.0402 val_acc:81.5085 val_loss:0.7518
[06/0115] | train_loss:0.0474 val_acc:83.2117 val_loss:0.7309
[06/0116] | train_loss:0.0443 val_acc:81.5085 val_loss:0.7403
[06/0117] | train_loss:0.0412 val_acc:82.2384 val_loss:0.7515
[06/0118] | train_loss:0.0446 val_acc:82.4818 val_loss:0.6926
[06/0119] | train_loss:0.0409 val_acc:81.7518 val_loss:0.7362
[06/0120] | train_loss:0.0398 val_acc:82.4818 val_loss:0.7425
[06/0121] | train_loss:0.0412 val_acc:82.2384 val_loss:0.7673
[06/0122] | train_loss:0.0434 val_acc:82.9684 val_loss:0.7122
[06/0123] | train_loss:0.0361 val_acc:80.5353 val_loss:0.749
[06/0124] | train_loss:0.0351 val_acc:80.292 val_loss:0.7689
[06/0125] | train_loss:0.043 val_acc:81.0219 val_loss:0.7613
[06/0126] | train_loss:0.0397 val_acc:79.562 val_loss:0.7869
Fold: [6/10] Test is finish !! 
 Test Metrics are: test_acc:81.2652 test_loss:0.6635fold [6/10] is start!!
[07/0001] | train_loss:0.6646 val_acc:45.7421 val_loss:0.695
model is saved at epoch 1!![07/0002] | train_loss:0.6256 val_acc:50.1217 val_loss:0.6903
model is saved at epoch 2!![07/0003] | train_loss:0.6081 val_acc:50.6083 val_loss:0.6933
model is saved at epoch 3!![07/0004] | train_loss:0.5831 val_acc:53.2847 val_loss:0.6759
model is saved at epoch 4!![07/0005] | train_loss:0.5663 val_acc:63.9903 val_loss:0.6305
model is saved at epoch 5!![07/0006] | train_loss:0.5368 val_acc:65.9367 val_loss:0.5791
model is saved at epoch 6!![07/0007] | train_loss:0.5194 val_acc:74.2092 val_loss:0.5197
model is saved at epoch 7!![07/0008] | train_loss:0.5044 val_acc:76.6423 val_loss:0.4934
model is saved at epoch 8!![07/0009] | train_loss:0.4895 val_acc:76.1557 val_loss:0.4812
[07/0010] | train_loss:0.476 val_acc:77.8589 val_loss:0.4744
model is saved at epoch 10!![07/0011] | train_loss:0.4736 val_acc:79.8054 val_loss:0.4686
model is saved at epoch 11!![07/0012] | train_loss:0.476 val_acc:74.4526 val_loss:0.4883
[07/0013] | train_loss:0.4544 val_acc:78.8321 val_loss:0.4805
[07/0014] | train_loss:0.4273 val_acc:79.562 val_loss:0.4561
[07/0015] | train_loss:0.4202 val_acc:77.6156 val_loss:0.4588
[07/0016] | train_loss:0.4053 val_acc:79.562 val_loss:0.4567
[07/0017] | train_loss:0.3889 val_acc:77.8589 val_loss:0.4873
[07/0018] | train_loss:0.3818 val_acc:78.1022 val_loss:0.4648
[07/0019] | train_loss:0.3792 val_acc:76.8856 val_loss:0.4735
[07/0020] | train_loss:0.3807 val_acc:75.6691 val_loss:0.4856
[07/0021] | train_loss:0.3609 val_acc:78.1022 val_loss:0.4669
[07/0022] | train_loss:0.3497 val_acc:78.5888 val_loss:0.4674
[07/0023] | train_loss:0.3414 val_acc:79.0754 val_loss:0.4794
[07/0024] | train_loss:0.3354 val_acc:79.0754 val_loss:0.4747
[07/0025] | train_loss:0.339 val_acc:76.8856 val_loss:0.5054
[07/0026] | train_loss:0.3345 val_acc:79.3187 val_loss:0.4661
[07/0027] | train_loss:0.3267 val_acc:78.8321 val_loss:0.464
[07/0028] | train_loss:0.3089 val_acc:78.1022 val_loss:0.4686
[07/0029] | train_loss:0.2991 val_acc:78.3455 val_loss:0.4737
[07/0030] | train_loss:0.2871 val_acc:80.0487 val_loss:0.4801
model is saved at epoch 30!![07/0031] | train_loss:0.3237 val_acc:75.9124 val_loss:0.5322
[07/0032] | train_loss:0.3113 val_acc:79.8054 val_loss:0.4658
[07/0033] | train_loss:0.2819 val_acc:79.0754 val_loss:0.4992
[07/0034] | train_loss:0.2785 val_acc:79.3187 val_loss:0.4901
[07/0035] | train_loss:0.2709 val_acc:81.2652 val_loss:0.4857
model is saved at epoch 35!![07/0036] | train_loss:0.2544 val_acc:79.562 val_loss:0.4977
[07/0037] | train_loss:0.2427 val_acc:76.1557 val_loss:0.5534
[07/0038] | train_loss:0.2487 val_acc:78.5888 val_loss:0.5186
[07/0039] | train_loss:0.2408 val_acc:78.1022 val_loss:0.5136
[07/0040] | train_loss:0.233 val_acc:78.5888 val_loss:0.5275
[07/0041] | train_loss:0.225 val_acc:77.8589 val_loss:0.5666
[07/0042] | train_loss:0.2178 val_acc:81.5085 val_loss:0.4976
model is saved at epoch 42!![07/0043] | train_loss:0.2144 val_acc:80.292 val_loss:0.5432
[07/0044] | train_loss:0.2203 val_acc:79.3187 val_loss:0.529
[07/0045] | train_loss:0.2198 val_acc:79.562 val_loss:0.5346
[07/0046] | train_loss:0.1999 val_acc:80.292 val_loss:0.5433
[07/0047] | train_loss:0.1882 val_acc:79.562 val_loss:0.523
[07/0048] | train_loss:0.1764 val_acc:79.3187 val_loss:0.5784
[07/0049] | train_loss:0.1796 val_acc:80.0487 val_loss:0.5503
[07/0050] | train_loss:0.1802 val_acc:78.1022 val_loss:0.6057
[07/0051] | train_loss:0.1919 val_acc:81.5085 val_loss:0.5526
[07/0052] | train_loss:0.1815 val_acc:78.3455 val_loss:0.595
[07/0053] | train_loss:0.199 val_acc:78.1022 val_loss:0.5726
[07/0054] | train_loss:0.1672 val_acc:79.8054 val_loss:0.5854
[07/0055] | train_loss:0.1867 val_acc:81.2652 val_loss:0.5319
[07/0056] | train_loss:0.156 val_acc:77.8589 val_loss:0.645
[07/0057] | train_loss:0.1517 val_acc:77.6156 val_loss:0.6041
[07/0058] | train_loss:0.1427 val_acc:79.3187 val_loss:0.6298
[07/0059] | train_loss:0.152 val_acc:79.562 val_loss:0.6063
[07/0060] | train_loss:0.1446 val_acc:80.0487 val_loss:0.6049
[07/0061] | train_loss:0.1417 val_acc:79.8054 val_loss:0.5842
[07/0062] | train_loss:0.1313 val_acc:79.0754 val_loss:0.6032
[07/0063] | train_loss:0.1335 val_acc:80.0487 val_loss:0.65
[07/0064] | train_loss:0.1447 val_acc:76.8856 val_loss:0.6176
[07/0065] | train_loss:0.1398 val_acc:81.2652 val_loss:0.5883
[07/0066] | train_loss:0.1131 val_acc:80.292 val_loss:0.6546
[07/0067] | train_loss:0.1051 val_acc:78.3455 val_loss:0.6741
[07/0068] | train_loss:0.1258 val_acc:79.0754 val_loss:0.6652
[07/0069] | train_loss:0.1425 val_acc:79.8054 val_loss:0.6169
[07/0070] | train_loss:0.1024 val_acc:79.0754 val_loss:0.6354
[07/0071] | train_loss:0.124 val_acc:80.0487 val_loss:0.685
[07/0072] | train_loss:0.1129 val_acc:80.292 val_loss:0.6139
[07/0073] | train_loss:0.1029 val_acc:80.7786 val_loss:0.6865
[07/0074] | train_loss:0.0997 val_acc:78.5888 val_loss:0.688
[07/0075] | train_loss:0.1039 val_acc:80.7786 val_loss:0.6518
[07/0076] | train_loss:0.1098 val_acc:80.5353 val_loss:0.64
[07/0077] | train_loss:0.0881 val_acc:77.3723 val_loss:0.7314
[07/0078] | train_loss:0.0993 val_acc:79.0754 val_loss:0.7158
[07/0079] | train_loss:0.0829 val_acc:77.8589 val_loss:0.7263
[07/0080] | train_loss:0.079 val_acc:81.7518 val_loss:0.7118
model is saved at epoch 80!![07/0081] | train_loss:0.0978 val_acc:81.5085 val_loss:0.6675
[07/0082] | train_loss:0.0838 val_acc:80.0487 val_loss:0.719
[07/0083] | train_loss:0.0939 val_acc:79.8054 val_loss:0.6718
[07/0084] | train_loss:0.111 val_acc:80.7786 val_loss:0.6687
[07/0085] | train_loss:0.1101 val_acc:78.5888 val_loss:0.7253
[07/0086] | train_loss:0.0861 val_acc:80.5353 val_loss:0.6648
[07/0087] | train_loss:0.0786 val_acc:79.0754 val_loss:0.7607
[07/0088] | train_loss:0.0763 val_acc:80.0487 val_loss:0.7535
[07/0089] | train_loss:0.073 val_acc:77.8589 val_loss:0.7699
[07/0090] | train_loss:0.0712 val_acc:81.9951 val_loss:0.7311
model is saved at epoch 90!![07/0091] | train_loss:0.0805 val_acc:80.292 val_loss:0.765
[07/0092] | train_loss:0.07 val_acc:80.5353 val_loss:0.7541
[07/0093] | train_loss:0.0826 val_acc:79.0754 val_loss:0.7473
[07/0094] | train_loss:0.0806 val_acc:80.292 val_loss:0.7185
[07/0095] | train_loss:0.0733 val_acc:78.8321 val_loss:0.759
[07/0096] | train_loss:0.0628 val_acc:79.562 val_loss:0.7972
[07/0097] | train_loss:0.0718 val_acc:79.3187 val_loss:0.8303
[07/0098] | train_loss:0.0684 val_acc:80.292 val_loss:0.7366
[07/0099] | train_loss:0.0721 val_acc:79.3187 val_loss:0.7769
[07/0100] | train_loss:0.0608 val_acc:79.0754 val_loss:0.7665
[07/0101] | train_loss:0.0638 val_acc:77.8589 val_loss:0.8049
[07/0102] | train_loss:0.0654 val_acc:78.1022 val_loss:0.7226
[07/0103] | train_loss:0.0664 val_acc:79.8054 val_loss:0.7464
[07/0104] | train_loss:0.0596 val_acc:81.0219 val_loss:0.8171
[07/0105] | train_loss:0.0694 val_acc:80.7786 val_loss:0.795
[07/0106] | train_loss:0.0705 val_acc:81.2652 val_loss:0.763
[07/0107] | train_loss:0.0526 val_acc:79.3187 val_loss:0.8123
[07/0108] | train_loss:0.0539 val_acc:80.292 val_loss:0.793
[07/0109] | train_loss:0.048 val_acc:79.0754 val_loss:0.812
[07/0110] | train_loss:0.0523 val_acc:78.3455 val_loss:0.9378
[07/0111] | train_loss:0.0638 val_acc:79.562 val_loss:0.8238
[07/0112] | train_loss:0.0488 val_acc:79.562 val_loss:0.7533
[07/0113] | train_loss:0.0577 val_acc:79.562 val_loss:0.8332
[07/0114] | train_loss:0.0561 val_acc:79.3187 val_loss:0.7862
[07/0115] | train_loss:0.0522 val_acc:81.7518 val_loss:0.7767
[07/0116] | train_loss:0.0506 val_acc:79.3187 val_loss:0.8623
[07/0117] | train_loss:0.0457 val_acc:80.7786 val_loss:0.8339
[07/0118] | train_loss:0.052 val_acc:80.0487 val_loss:0.8309
[07/0119] | train_loss:0.0553 val_acc:77.8589 val_loss:0.8777
[07/0120] | train_loss:0.047 val_acc:79.3187 val_loss:0.8433
[07/0121] | train_loss:0.0457 val_acc:79.3187 val_loss:0.8683
[07/0122] | train_loss:0.0446 val_acc:79.562 val_loss:0.8847
[07/0123] | train_loss:0.0473 val_acc:80.5353 val_loss:0.7733
[07/0124] | train_loss:0.0535 val_acc:80.7786 val_loss:0.8236
[07/0125] | train_loss:0.0431 val_acc:80.0487 val_loss:0.829
[07/0126] | train_loss:0.0354 val_acc:78.3455 val_loss:0.904
[07/0127] | train_loss:0.0389 val_acc:77.8589 val_loss:0.8874
[07/0128] | train_loss:0.045 val_acc:79.562 val_loss:0.9034
[07/0129] | train_loss:0.0508 val_acc:80.292 val_loss:0.8829
[07/0130] | train_loss:0.0441 val_acc:80.0487 val_loss:0.8722
[07/0131] | train_loss:0.0452 val_acc:80.292 val_loss:0.8962
[07/0132] | train_loss:0.0478 val_acc:78.8321 val_loss:0.9464
[07/0133] | train_loss:0.0542 val_acc:80.0487 val_loss:0.851
[07/0134] | train_loss:0.0548 val_acc:81.5085 val_loss:0.8298
[07/0135] | train_loss:0.0621 val_acc:79.0754 val_loss:0.8452
[07/0136] | train_loss:0.0572 val_acc:80.5353 val_loss:0.7839
[07/0137] | train_loss:0.048 val_acc:79.562 val_loss:0.8398
[07/0138] | train_loss:0.0422 val_acc:77.129 val_loss:0.8863
[07/0139] | train_loss:0.044 val_acc:80.292 val_loss:0.8364
[07/0140] | train_loss:0.0454 val_acc:79.562 val_loss:0.8872
[07/0141] | train_loss:0.0356 val_acc:78.5888 val_loss:0.9354
Fold: [7/10] Test is finish !! 
 Test Metrics are: test_acc:80.0487 test_loss:0.7344fold [7/10] is start!!
[08/0001] | train_loss:0.6636 val_acc:50.6083 val_loss:0.7152
model is saved at epoch 1!![08/0002] | train_loss:0.6324 val_acc:50.6083 val_loss:0.7357
[08/0003] | train_loss:0.6185 val_acc:49.3917 val_loss:0.709
[08/0004] | train_loss:0.6115 val_acc:52.5547 val_loss:0.6887
model is saved at epoch 4!![08/0005] | train_loss:0.6023 val_acc:56.4477 val_loss:0.657
model is saved at epoch 5!![08/0006] | train_loss:0.5975 val_acc:64.7202 val_loss:0.6006
model is saved at epoch 6!![08/0007] | train_loss:0.5763 val_acc:69.0998 val_loss:0.5597
model is saved at epoch 7!![08/0008] | train_loss:0.572 val_acc:72.0195 val_loss:0.5424
model is saved at epoch 8!![08/0009] | train_loss:0.5586 val_acc:72.7494 val_loss:0.529
model is saved at epoch 9!![08/0010] | train_loss:0.5518 val_acc:75.6691 val_loss:0.5263
model is saved at epoch 10!![08/0011] | train_loss:0.5372 val_acc:74.2092 val_loss:0.5195
[08/0012] | train_loss:0.5406 val_acc:76.399 val_loss:0.5102
model is saved at epoch 12!![08/0013] | train_loss:0.5259 val_acc:72.0195 val_loss:0.5374
[08/0014] | train_loss:0.5263 val_acc:74.6959 val_loss:0.515
[08/0015] | train_loss:0.5066 val_acc:76.399 val_loss:0.5036
[08/0016] | train_loss:0.51 val_acc:73.7226 val_loss:0.5111
[08/0017] | train_loss:0.4985 val_acc:77.129 val_loss:0.4977
model is saved at epoch 17!![08/0018] | train_loss:0.4888 val_acc:76.1557 val_loss:0.5031
[08/0019] | train_loss:0.4764 val_acc:75.4258 val_loss:0.4993
[08/0020] | train_loss:0.4671 val_acc:76.6423 val_loss:0.495
[08/0021] | train_loss:0.4625 val_acc:74.2092 val_loss:0.4993
[08/0022] | train_loss:0.4578 val_acc:76.8856 val_loss:0.4942
[08/0023] | train_loss:0.4479 val_acc:77.8589 val_loss:0.4872
model is saved at epoch 23!![08/0024] | train_loss:0.4436 val_acc:76.399 val_loss:0.494
[08/0025] | train_loss:0.4376 val_acc:77.3723 val_loss:0.4911
[08/0026] | train_loss:0.4359 val_acc:77.8589 val_loss:0.4864
[08/0027] | train_loss:0.4358 val_acc:80.0487 val_loss:0.4734
model is saved at epoch 27!![08/0028] | train_loss:0.4259 val_acc:76.399 val_loss:0.4958
[08/0029] | train_loss:0.4206 val_acc:76.399 val_loss:0.4911
[08/0030] | train_loss:0.4187 val_acc:76.8856 val_loss:0.4889
[08/0031] | train_loss:0.4162 val_acc:77.6156 val_loss:0.4749
[08/0032] | train_loss:0.4117 val_acc:78.1022 val_loss:0.4806
[08/0033] | train_loss:0.4072 val_acc:75.4258 val_loss:0.5259
[08/0034] | train_loss:0.4046 val_acc:76.6423 val_loss:0.4942
[08/0035] | train_loss:0.4118 val_acc:77.3723 val_loss:0.5102
[08/0036] | train_loss:0.4059 val_acc:77.129 val_loss:0.4874
[08/0037] | train_loss:0.4051 val_acc:78.8321 val_loss:0.476
[08/0038] | train_loss:0.3922 val_acc:79.8054 val_loss:0.4777
[08/0039] | train_loss:0.3886 val_acc:79.3187 val_loss:0.4587
[08/0040] | train_loss:0.3796 val_acc:76.6423 val_loss:0.4652
[08/0041] | train_loss:0.3927 val_acc:77.3723 val_loss:0.5073
[08/0042] | train_loss:0.3812 val_acc:78.8321 val_loss:0.4946
[08/0043] | train_loss:0.3799 val_acc:79.8054 val_loss:0.458
[08/0044] | train_loss:0.3793 val_acc:77.3723 val_loss:0.4618
[08/0045] | train_loss:0.3778 val_acc:77.6156 val_loss:0.4668
[08/0046] | train_loss:0.365 val_acc:80.0487 val_loss:0.4629
[08/0047] | train_loss:0.3762 val_acc:79.3187 val_loss:0.4577
[08/0048] | train_loss:0.3632 val_acc:80.0487 val_loss:0.4647
[08/0049] | train_loss:0.3789 val_acc:79.0754 val_loss:0.456
[08/0050] | train_loss:0.36 val_acc:80.0487 val_loss:0.4748
[08/0051] | train_loss:0.3581 val_acc:77.8589 val_loss:0.4606
[08/0052] | train_loss:0.3507 val_acc:80.292 val_loss:0.4615
model is saved at epoch 52!![08/0053] | train_loss:0.3533 val_acc:79.3187 val_loss:0.4567
[08/0054] | train_loss:0.3376 val_acc:77.3723 val_loss:0.5112
[08/0055] | train_loss:0.3386 val_acc:79.3187 val_loss:0.4879
[08/0056] | train_loss:0.3416 val_acc:79.8054 val_loss:0.4679
[08/0057] | train_loss:0.3408 val_acc:79.8054 val_loss:0.4607
[08/0058] | train_loss:0.3394 val_acc:76.6423 val_loss:0.4859
[08/0059] | train_loss:0.3302 val_acc:75.9124 val_loss:0.4878
[08/0060] | train_loss:0.333 val_acc:80.5353 val_loss:0.47
model is saved at epoch 60!![08/0061] | train_loss:0.3208 val_acc:78.3455 val_loss:0.4957
[08/0062] | train_loss:0.3362 val_acc:79.3187 val_loss:0.4716
[08/0063] | train_loss:0.3173 val_acc:81.0219 val_loss:0.478
model is saved at epoch 63!![08/0064] | train_loss:0.3164 val_acc:76.8856 val_loss:0.4949
[08/0065] | train_loss:0.3168 val_acc:77.6156 val_loss:0.4668
[08/0066] | train_loss:0.3198 val_acc:79.0754 val_loss:0.4628
[08/0067] | train_loss:0.3124 val_acc:78.5888 val_loss:0.4944
[08/0068] | train_loss:0.3203 val_acc:78.1022 val_loss:0.5056
[08/0069] | train_loss:0.3198 val_acc:80.7786 val_loss:0.4539
[08/0070] | train_loss:0.3105 val_acc:79.8054 val_loss:0.4796
[08/0071] | train_loss:0.3033 val_acc:77.6156 val_loss:0.4911
[08/0072] | train_loss:0.3114 val_acc:77.129 val_loss:0.4754
[08/0073] | train_loss:0.2861 val_acc:77.6156 val_loss:0.4688
[08/0074] | train_loss:0.301 val_acc:77.6156 val_loss:0.4999
[08/0075] | train_loss:0.289 val_acc:77.6156 val_loss:0.4889
[08/0076] | train_loss:0.29 val_acc:78.5888 val_loss:0.5087
[08/0077] | train_loss:0.2963 val_acc:79.562 val_loss:0.4891
[08/0078] | train_loss:0.302 val_acc:80.0487 val_loss:0.47
[08/0079] | train_loss:0.2831 val_acc:79.3187 val_loss:0.5121
[08/0080] | train_loss:0.2995 val_acc:80.292 val_loss:0.4586
[08/0081] | train_loss:0.28 val_acc:79.3187 val_loss:0.5012
[08/0082] | train_loss:0.2934 val_acc:79.0754 val_loss:0.4701
[08/0083] | train_loss:0.2873 val_acc:79.562 val_loss:0.4784
[08/0084] | train_loss:0.2876 val_acc:80.0487 val_loss:0.4683
[08/0085] | train_loss:0.2748 val_acc:80.5353 val_loss:0.47
[08/0086] | train_loss:0.2864 val_acc:77.8589 val_loss:0.5183
[08/0087] | train_loss:0.2847 val_acc:77.6156 val_loss:0.4841
[08/0088] | train_loss:0.2751 val_acc:77.8589 val_loss:0.4924
[08/0089] | train_loss:0.2845 val_acc:76.6423 val_loss:0.5237
[08/0090] | train_loss:0.2677 val_acc:78.3455 val_loss:0.4913
[08/0091] | train_loss:0.274 val_acc:78.8321 val_loss:0.4822
[08/0092] | train_loss:0.2852 val_acc:77.129 val_loss:0.4851
[08/0093] | train_loss:0.2616 val_acc:78.5888 val_loss:0.4886
[08/0094] | train_loss:0.2613 val_acc:80.7786 val_loss:0.4685
[08/0095] | train_loss:0.2563 val_acc:80.292 val_loss:0.491
[08/0096] | train_loss:0.2607 val_acc:79.8054 val_loss:0.4788
[08/0097] | train_loss:0.2626 val_acc:79.3187 val_loss:0.4884
[08/0098] | train_loss:0.2759 val_acc:80.7786 val_loss:0.4794
[08/0099] | train_loss:0.2638 val_acc:80.7786 val_loss:0.4642
[08/0100] | train_loss:0.2669 val_acc:80.7786 val_loss:0.4859
[08/0101] | train_loss:0.2672 val_acc:80.7786 val_loss:0.4809
[08/0102] | train_loss:0.267 val_acc:79.8054 val_loss:0.4719
[08/0103] | train_loss:0.2536 val_acc:81.2652 val_loss:0.4796
model is saved at epoch 103!![08/0104] | train_loss:0.2446 val_acc:81.0219 val_loss:0.4924
[08/0105] | train_loss:0.254 val_acc:76.8856 val_loss:0.5029
[08/0106] | train_loss:0.2561 val_acc:80.7786 val_loss:0.5054
[08/0107] | train_loss:0.2432 val_acc:78.8321 val_loss:0.4873
[08/0108] | train_loss:0.2509 val_acc:79.8054 val_loss:0.4841
[08/0109] | train_loss:0.2409 val_acc:81.7518 val_loss:0.4908
model is saved at epoch 109!![08/0110] | train_loss:0.2467 val_acc:79.0754 val_loss:0.4933
[08/0111] | train_loss:0.2448 val_acc:80.7786 val_loss:0.4711
[08/0112] | train_loss:0.2459 val_acc:79.3187 val_loss:0.5163
[08/0113] | train_loss:0.2477 val_acc:78.1022 val_loss:0.5027
[08/0114] | train_loss:0.2537 val_acc:79.562 val_loss:0.4973
[08/0115] | train_loss:0.2559 val_acc:80.0487 val_loss:0.4605
[08/0116] | train_loss:0.2418 val_acc:80.0487 val_loss:0.4928
[08/0117] | train_loss:0.2434 val_acc:80.7786 val_loss:0.4628
[08/0118] | train_loss:0.2414 val_acc:79.562 val_loss:0.4884
[08/0119] | train_loss:0.2407 val_acc:79.562 val_loss:0.503
[08/0120] | train_loss:0.2451 val_acc:79.562 val_loss:0.4702
[08/0121] | train_loss:0.2434 val_acc:79.562 val_loss:0.4736
[08/0122] | train_loss:0.2429 val_acc:78.5888 val_loss:0.4819
[08/0123] | train_loss:0.2439 val_acc:80.5353 val_loss:0.4842
[08/0124] | train_loss:0.2275 val_acc:77.6156 val_loss:0.505
[08/0125] | train_loss:0.246 val_acc:80.0487 val_loss:0.4692
[08/0126] | train_loss:0.235 val_acc:78.8321 val_loss:0.4923
[08/0127] | train_loss:0.2258 val_acc:80.7786 val_loss:0.4908
[08/0128] | train_loss:0.2199 val_acc:81.0219 val_loss:0.5061
[08/0129] | train_loss:0.2383 val_acc:79.8054 val_loss:0.4938
[08/0130] | train_loss:0.2493 val_acc:81.7518 val_loss:0.4703
[08/0131] | train_loss:0.2229 val_acc:82.4818 val_loss:0.4988
model is saved at epoch 131!![08/0132] | train_loss:0.2264 val_acc:78.3455 val_loss:0.5129
[08/0133] | train_loss:0.2129 val_acc:81.9951 val_loss:0.4942
[08/0134] | train_loss:0.2275 val_acc:80.292 val_loss:0.4775
[08/0135] | train_loss:0.217 val_acc:82.2384 val_loss:0.5279
[08/0136] | train_loss:0.2137 val_acc:80.0487 val_loss:0.5125
[08/0137] | train_loss:0.2217 val_acc:79.0754 val_loss:0.4899
[08/0138] | train_loss:0.2289 val_acc:80.7786 val_loss:0.505
[08/0139] | train_loss:0.2127 val_acc:80.0487 val_loss:0.5095
[08/0140] | train_loss:0.2258 val_acc:79.562 val_loss:0.5523
[08/0141] | train_loss:0.2166 val_acc:78.8321 val_loss:0.512
[08/0142] | train_loss:0.2145 val_acc:79.562 val_loss:0.5314
[08/0143] | train_loss:0.2307 val_acc:80.5353 val_loss:0.51
[08/0144] | train_loss:0.2024 val_acc:79.562 val_loss:0.5431
[08/0145] | train_loss:0.1997 val_acc:78.8321 val_loss:0.56
[08/0146] | train_loss:0.2098 val_acc:81.7518 val_loss:0.5544
[08/0147] | train_loss:0.2198 val_acc:80.5353 val_loss:0.5009
[08/0148] | train_loss:0.208 val_acc:79.0754 val_loss:0.5675
[08/0149] | train_loss:0.2114 val_acc:79.8054 val_loss:0.523
[08/0150] | train_loss:0.2072 val_acc:79.3187 val_loss:0.555
[08/0151] | train_loss:0.1931 val_acc:78.8321 val_loss:0.5718
[08/0152] | train_loss:0.1973 val_acc:81.0219 val_loss:0.5344
[08/0153] | train_loss:0.2074 val_acc:80.0487 val_loss:0.5598
[08/0154] | train_loss:0.2152 val_acc:79.562 val_loss:0.5672
[08/0155] | train_loss:0.2048 val_acc:80.5353 val_loss:0.5779
[08/0156] | train_loss:0.1959 val_acc:77.6156 val_loss:0.5692
[08/0157] | train_loss:0.1934 val_acc:78.5888 val_loss:0.5347
[08/0158] | train_loss:0.1903 val_acc:80.7786 val_loss:0.5553
[08/0159] | train_loss:0.2146 val_acc:79.3187 val_loss:0.5243
[08/0160] | train_loss:0.1927 val_acc:79.3187 val_loss:0.5784
[08/0161] | train_loss:0.1817 val_acc:79.0754 val_loss:0.5993
[08/0162] | train_loss:0.1859 val_acc:77.8589 val_loss:0.5774
[08/0163] | train_loss:0.1908 val_acc:77.8589 val_loss:0.5879
[08/0164] | train_loss:0.1973 val_acc:79.3187 val_loss:0.5726
[08/0165] | train_loss:0.2024 val_acc:79.562 val_loss:0.5556
[08/0166] | train_loss:0.1802 val_acc:79.3187 val_loss:0.5912
[08/0167] | train_loss:0.1978 val_acc:81.0219 val_loss:0.5555
[08/0168] | train_loss:0.2031 val_acc:79.8054 val_loss:0.5798
[08/0169] | train_loss:0.1874 val_acc:77.6156 val_loss:0.5834
[08/0170] | train_loss:0.185 val_acc:79.0754 val_loss:0.6203
[08/0171] | train_loss:0.2178 val_acc:78.8321 val_loss:0.5317
[08/0172] | train_loss:0.1993 val_acc:79.0754 val_loss:0.5433
[08/0173] | train_loss:0.1833 val_acc:79.3187 val_loss:0.5577
[08/0174] | train_loss:0.1889 val_acc:78.3455 val_loss:0.5685
[08/0175] | train_loss:0.1982 val_acc:78.5888 val_loss:0.5906
[08/0176] | train_loss:0.1844 val_acc:81.0219 val_loss:0.5714
[08/0177] | train_loss:0.1871 val_acc:79.3187 val_loss:0.5986
[08/0178] | train_loss:0.2009 val_acc:79.0754 val_loss:0.5748
[08/0179] | train_loss:0.197 val_acc:76.399 val_loss:0.5772
[08/0180] | train_loss:0.1794 val_acc:79.562 val_loss:0.5939
[08/0181] | train_loss:0.1762 val_acc:79.8054 val_loss:0.5825
[08/0182] | train_loss:0.1763 val_acc:79.8054 val_loss:0.5818
Fold: [8/10] Test is finish !! 
 Test Metrics are: test_acc:83.2117 test_loss:0.5213fold [8/10] is start!!
[09/0001] | train_loss:0.6706 val_acc:53.0414 val_loss:0.6854
model is saved at epoch 1!![09/0002] | train_loss:0.6351 val_acc:61.3139 val_loss:0.685
model is saved at epoch 2!![09/0003] | train_loss:0.6146 val_acc:57.9075 val_loss:0.6731
[09/0004] | train_loss:0.5938 val_acc:58.3942 val_loss:0.6615
[09/0005] | train_loss:0.5788 val_acc:62.5304 val_loss:0.6218
model is saved at epoch 5!![09/0006] | train_loss:0.5536 val_acc:70.073 val_loss:0.5591
model is saved at epoch 6!![09/0007] | train_loss:0.5325 val_acc:73.7226 val_loss:0.5134
model is saved at epoch 7!![09/0008] | train_loss:0.519 val_acc:77.3723 val_loss:0.4772
model is saved at epoch 8!![09/0009] | train_loss:0.5003 val_acc:78.1022 val_loss:0.4988
model is saved at epoch 9!![09/0010] | train_loss:0.4819 val_acc:78.8321 val_loss:0.4612
model is saved at epoch 10!![09/0011] | train_loss:0.4709 val_acc:81.5085 val_loss:0.4444
model is saved at epoch 11!![09/0012] | train_loss:0.4724 val_acc:81.0219 val_loss:0.4565
[09/0013] | train_loss:0.4528 val_acc:80.5353 val_loss:0.4617
[09/0014] | train_loss:0.4445 val_acc:81.2652 val_loss:0.4294
[09/0015] | train_loss:0.4307 val_acc:81.0219 val_loss:0.4223
[09/0016] | train_loss:0.4286 val_acc:80.5353 val_loss:0.4541
[09/0017] | train_loss:0.4182 val_acc:80.7786 val_loss:0.4259
[09/0018] | train_loss:0.433 val_acc:82.9684 val_loss:0.419
model is saved at epoch 18!![09/0019] | train_loss:0.4173 val_acc:82.4818 val_loss:0.4456
[09/0020] | train_loss:0.4043 val_acc:81.2652 val_loss:0.4105
[09/0021] | train_loss:0.4015 val_acc:81.5085 val_loss:0.443
[09/0022] | train_loss:0.3949 val_acc:79.8054 val_loss:0.4644
[09/0023] | train_loss:0.3804 val_acc:80.7786 val_loss:0.4154
[09/0024] | train_loss:0.3743 val_acc:80.7786 val_loss:0.4509
[09/0025] | train_loss:0.3744 val_acc:80.292 val_loss:0.4481
[09/0026] | train_loss:0.3615 val_acc:82.4818 val_loss:0.41
[09/0027] | train_loss:0.3665 val_acc:81.5085 val_loss:0.4551
[09/0028] | train_loss:0.3506 val_acc:81.2652 val_loss:0.4427
[09/0029] | train_loss:0.3383 val_acc:80.7786 val_loss:0.4191
[09/0030] | train_loss:0.3443 val_acc:81.0219 val_loss:0.4328
[09/0031] | train_loss:0.3454 val_acc:81.2652 val_loss:0.4301
[09/0032] | train_loss:0.3331 val_acc:81.2652 val_loss:0.4376
[09/0033] | train_loss:0.3266 val_acc:80.7786 val_loss:0.4315
[09/0034] | train_loss:0.321 val_acc:81.5085 val_loss:0.4483
[09/0035] | train_loss:0.3155 val_acc:78.5888 val_loss:0.5064
[09/0036] | train_loss:0.3404 val_acc:77.8589 val_loss:0.4688
[09/0037] | train_loss:0.3208 val_acc:80.0487 val_loss:0.4535
[09/0038] | train_loss:0.3034 val_acc:81.9951 val_loss:0.4409
[09/0039] | train_loss:0.2924 val_acc:80.292 val_loss:0.4887
[09/0040] | train_loss:0.3028 val_acc:81.2652 val_loss:0.4782
[09/0041] | train_loss:0.3017 val_acc:80.5353 val_loss:0.4498
[09/0042] | train_loss:0.2939 val_acc:80.5353 val_loss:0.4384
[09/0043] | train_loss:0.3018 val_acc:77.8589 val_loss:0.5219
[09/0044] | train_loss:0.2865 val_acc:81.5085 val_loss:0.4366
[09/0045] | train_loss:0.2737 val_acc:81.5085 val_loss:0.4515
[09/0046] | train_loss:0.2831 val_acc:81.7518 val_loss:0.4447
[09/0047] | train_loss:0.263 val_acc:79.0754 val_loss:0.5014
[09/0048] | train_loss:0.2675 val_acc:80.5353 val_loss:0.4463
[09/0049] | train_loss:0.2456 val_acc:78.8321 val_loss:0.5179
[09/0050] | train_loss:0.253 val_acc:78.8321 val_loss:0.4668
[09/0051] | train_loss:0.2641 val_acc:80.292 val_loss:0.4882
[09/0052] | train_loss:0.2492 val_acc:81.0219 val_loss:0.4815
[09/0053] | train_loss:0.2411 val_acc:80.292 val_loss:0.4452
[09/0054] | train_loss:0.2685 val_acc:79.562 val_loss:0.5186
[09/0055] | train_loss:0.2579 val_acc:80.7786 val_loss:0.4693
[09/0056] | train_loss:0.2392 val_acc:81.2652 val_loss:0.4789
[09/0057] | train_loss:0.2346 val_acc:80.7786 val_loss:0.4679
[09/0058] | train_loss:0.2172 val_acc:81.5085 val_loss:0.4717
[09/0059] | train_loss:0.229 val_acc:80.0487 val_loss:0.4831
[09/0060] | train_loss:0.2091 val_acc:80.292 val_loss:0.5064
[09/0061] | train_loss:0.2204 val_acc:81.2652 val_loss:0.5083
[09/0062] | train_loss:0.2194 val_acc:81.5085 val_loss:0.4752
[09/0063] | train_loss:0.221 val_acc:80.292 val_loss:0.4867
[09/0064] | train_loss:0.2123 val_acc:83.455 val_loss:0.4517
model is saved at epoch 64!![09/0065] | train_loss:0.2091 val_acc:78.1022 val_loss:0.4932
[09/0066] | train_loss:0.2029 val_acc:81.9951 val_loss:0.4894
[09/0067] | train_loss:0.206 val_acc:78.8321 val_loss:0.4809
[09/0068] | train_loss:0.1973 val_acc:81.9951 val_loss:0.5052
[09/0069] | train_loss:0.1934 val_acc:80.292 val_loss:0.5169
[09/0070] | train_loss:0.1814 val_acc:80.7786 val_loss:0.5085
[09/0071] | train_loss:0.1851 val_acc:81.0219 val_loss:0.5319
[09/0072] | train_loss:0.1785 val_acc:80.292 val_loss:0.5787
[09/0073] | train_loss:0.189 val_acc:81.0219 val_loss:0.524
[09/0074] | train_loss:0.1745 val_acc:80.7786 val_loss:0.4979
[09/0075] | train_loss:0.1791 val_acc:80.7786 val_loss:0.489
[09/0076] | train_loss:0.1822 val_acc:80.292 val_loss:0.518
[09/0077] | train_loss:0.1663 val_acc:81.7518 val_loss:0.5298
[09/0078] | train_loss:0.1766 val_acc:78.3455 val_loss:0.559
[09/0079] | train_loss:0.1613 val_acc:80.292 val_loss:0.5588
[09/0080] | train_loss:0.1766 val_acc:81.0219 val_loss:0.5392
[09/0081] | train_loss:0.1767 val_acc:79.562 val_loss:0.6423
[09/0082] | train_loss:0.1939 val_acc:80.7786 val_loss:0.4974
[09/0083] | train_loss:0.1612 val_acc:79.0754 val_loss:0.5721
[09/0084] | train_loss:0.1643 val_acc:79.0754 val_loss:0.5465
[09/0085] | train_loss:0.1624 val_acc:79.3187 val_loss:0.5367
[09/0086] | train_loss:0.1505 val_acc:80.0487 val_loss:0.5928
[09/0087] | train_loss:0.1439 val_acc:81.2652 val_loss:0.5556
[09/0088] | train_loss:0.1431 val_acc:82.4818 val_loss:0.588
[09/0089] | train_loss:0.143 val_acc:81.7518 val_loss:0.534
[09/0090] | train_loss:0.1683 val_acc:80.5353 val_loss:0.5361
[09/0091] | train_loss:0.1322 val_acc:81.5085 val_loss:0.6034
[09/0092] | train_loss:0.1206 val_acc:80.5353 val_loss:0.641
[09/0093] | train_loss:0.14 val_acc:80.0487 val_loss:0.6092
[09/0094] | train_loss:0.1362 val_acc:79.3187 val_loss:0.6254
[09/0095] | train_loss:0.118 val_acc:81.5085 val_loss:0.5959
[09/0096] | train_loss:0.1171 val_acc:79.3187 val_loss:0.6189
[09/0097] | train_loss:0.1326 val_acc:78.3455 val_loss:0.6459
[09/0098] | train_loss:0.1362 val_acc:80.7786 val_loss:0.5963
[09/0099] | train_loss:0.1244 val_acc:80.0487 val_loss:0.673
[09/0100] | train_loss:0.1329 val_acc:79.0754 val_loss:0.6621
[09/0101] | train_loss:0.1442 val_acc:80.0487 val_loss:0.6188
[09/0102] | train_loss:0.1168 val_acc:80.7786 val_loss:0.656
[09/0103] | train_loss:0.1203 val_acc:77.3723 val_loss:0.6741
[09/0104] | train_loss:0.1434 val_acc:80.5353 val_loss:0.6059
[09/0105] | train_loss:0.1496 val_acc:80.292 val_loss:0.6477
[09/0106] | train_loss:0.1242 val_acc:78.5888 val_loss:0.6384
[09/0107] | train_loss:0.1026 val_acc:79.0754 val_loss:0.6336
[09/0108] | train_loss:0.1074 val_acc:79.8054 val_loss:0.6323
[09/0109] | train_loss:0.1193 val_acc:79.562 val_loss:0.6485
[09/0110] | train_loss:0.1226 val_acc:81.5085 val_loss:0.6535
[09/0111] | train_loss:0.0997 val_acc:79.0754 val_loss:0.6939
[09/0112] | train_loss:0.1116 val_acc:80.292 val_loss:0.6496
[09/0113] | train_loss:0.0967 val_acc:79.8054 val_loss:0.6885
[09/0114] | train_loss:0.1007 val_acc:80.5353 val_loss:0.6593
[09/0115] | train_loss:0.1045 val_acc:78.8321 val_loss:0.7102
Fold: [9/10] Test is finish !! 
 Test Metrics are: test_acc:80.0487 test_loss:0.5412fold [9/10] is start!!
[10/0001] | train_loss:0.6703 val_acc:50.6083 val_loss:0.6925
model is saved at epoch 1!![10/0002] | train_loss:0.6355 val_acc:48.1752 val_loss:0.6959
[10/0003] | train_loss:0.621 val_acc:48.6618 val_loss:0.6946
[10/0004] | train_loss:0.6005 val_acc:49.3917 val_loss:0.6769
[10/0005] | train_loss:0.5864 val_acc:62.0438 val_loss:0.6296
model is saved at epoch 5!![10/0006] | train_loss:0.5726 val_acc:72.0195 val_loss:0.5704
model is saved at epoch 6!![10/0007] | train_loss:0.5542 val_acc:72.5061 val_loss:0.5432
model is saved at epoch 7!![10/0008] | train_loss:0.5337 val_acc:74.6959 val_loss:0.5182
model is saved at epoch 8!![10/0009] | train_loss:0.5287 val_acc:73.7226 val_loss:0.5207
[10/0010] | train_loss:0.5247 val_acc:77.129 val_loss:0.523
model is saved at epoch 10!![10/0011] | train_loss:0.5192 val_acc:74.9392 val_loss:0.5079
[10/0012] | train_loss:0.5066 val_acc:76.399 val_loss:0.4999
[10/0013] | train_loss:0.4999 val_acc:73.9659 val_loss:0.5078
[10/0014] | train_loss:0.4985 val_acc:76.399 val_loss:0.4933
[10/0015] | train_loss:0.4935 val_acc:77.3723 val_loss:0.4972
model is saved at epoch 15!![10/0016] | train_loss:0.474 val_acc:76.1557 val_loss:0.5059
[10/0017] | train_loss:0.4789 val_acc:77.3723 val_loss:0.4952
[10/0018] | train_loss:0.4787 val_acc:77.129 val_loss:0.5219
[10/0019] | train_loss:0.4691 val_acc:76.399 val_loss:0.4969
[10/0020] | train_loss:0.4619 val_acc:78.1022 val_loss:0.4913
model is saved at epoch 20!![10/0021] | train_loss:0.4524 val_acc:79.0754 val_loss:0.4883
model is saved at epoch 21!![10/0022] | train_loss:0.4468 val_acc:77.8589 val_loss:0.4786
[10/0023] | train_loss:0.4383 val_acc:79.3187 val_loss:0.4879
model is saved at epoch 23!![10/0024] | train_loss:0.4317 val_acc:80.5353 val_loss:0.4811
model is saved at epoch 24!![10/0025] | train_loss:0.4522 val_acc:76.399 val_loss:0.4882
[10/0026] | train_loss:0.4655 val_acc:80.5353 val_loss:0.4747
[10/0027] | train_loss:0.4232 val_acc:78.1022 val_loss:0.4966
[10/0028] | train_loss:0.4184 val_acc:80.7786 val_loss:0.4738
model is saved at epoch 28!![10/0029] | train_loss:0.4166 val_acc:79.3187 val_loss:0.4871
[10/0030] | train_loss:0.4135 val_acc:79.562 val_loss:0.4791
[10/0031] | train_loss:0.4165 val_acc:80.5353 val_loss:0.4748
[10/0032] | train_loss:0.3994 val_acc:80.0487 val_loss:0.4741
[10/0033] | train_loss:0.4054 val_acc:77.3723 val_loss:0.4933
[10/0034] | train_loss:0.4066 val_acc:80.5353 val_loss:0.4828
[10/0035] | train_loss:0.3925 val_acc:80.0487 val_loss:0.4699
[10/0036] | train_loss:0.3964 val_acc:77.3723 val_loss:0.4836
[10/0037] | train_loss:0.4076 val_acc:78.5888 val_loss:0.4725
[10/0038] | train_loss:0.4117 val_acc:79.3187 val_loss:0.4677
[10/0039] | train_loss:0.3875 val_acc:80.0487 val_loss:0.464
[10/0040] | train_loss:0.3863 val_acc:79.0754 val_loss:0.4698
[10/0041] | train_loss:0.3837 val_acc:79.3187 val_loss:0.4582
[10/0042] | train_loss:0.3882 val_acc:78.3455 val_loss:0.4809
[10/0043] | train_loss:0.3796 val_acc:79.8054 val_loss:0.47
[10/0044] | train_loss:0.3832 val_acc:79.8054 val_loss:0.458
[10/0045] | train_loss:0.3616 val_acc:81.0219 val_loss:0.4751
model is saved at epoch 45!![10/0046] | train_loss:0.3663 val_acc:80.292 val_loss:0.4664
[10/0047] | train_loss:0.3634 val_acc:80.7786 val_loss:0.4641
[10/0048] | train_loss:0.369 val_acc:79.0754 val_loss:0.46
[10/0049] | train_loss:0.3624 val_acc:81.2652 val_loss:0.4563
model is saved at epoch 49!![10/0050] | train_loss:0.3465 val_acc:80.292 val_loss:0.4786
[10/0051] | train_loss:0.3442 val_acc:79.562 val_loss:0.4636
[10/0052] | train_loss:0.3465 val_acc:81.9951 val_loss:0.4653
model is saved at epoch 52!![10/0053] | train_loss:0.3416 val_acc:81.0219 val_loss:0.463
[10/0054] | train_loss:0.344 val_acc:81.0219 val_loss:0.4702
[10/0055] | train_loss:0.3603 val_acc:81.9951 val_loss:0.4464
[10/0056] | train_loss:0.3333 val_acc:80.292 val_loss:0.4703
[10/0057] | train_loss:0.3252 val_acc:80.0487 val_loss:0.4832
[10/0058] | train_loss:0.3344 val_acc:79.0754 val_loss:0.5213
[10/0059] | train_loss:0.3253 val_acc:79.3187 val_loss:0.4848
[10/0060] | train_loss:0.3225 val_acc:79.8054 val_loss:0.4809
[10/0061] | train_loss:0.3246 val_acc:79.562 val_loss:0.51
[10/0062] | train_loss:0.3325 val_acc:79.8054 val_loss:0.5368
[10/0063] | train_loss:0.3277 val_acc:81.0219 val_loss:0.4763
[10/0064] | train_loss:0.302 val_acc:81.2652 val_loss:0.4926
[10/0065] | train_loss:0.3131 val_acc:79.3187 val_loss:0.4829
[10/0066] | train_loss:0.323 val_acc:80.0487 val_loss:0.47
[10/0067] | train_loss:0.3069 val_acc:80.292 val_loss:0.5066
[10/0068] | train_loss:0.3123 val_acc:79.3187 val_loss:0.5132
[10/0069] | train_loss:0.3043 val_acc:80.0487 val_loss:0.4842
[10/0070] | train_loss:0.3362 val_acc:78.3455 val_loss:0.4921
[10/0071] | train_loss:0.306 val_acc:80.0487 val_loss:0.4885
[10/0072] | train_loss:0.2988 val_acc:81.5085 val_loss:0.4769
[10/0073] | train_loss:0.295 val_acc:79.3187 val_loss:0.4849
[10/0074] | train_loss:0.3062 val_acc:80.0487 val_loss:0.5173
[10/0075] | train_loss:0.3082 val_acc:81.0219 val_loss:0.455
[10/0076] | train_loss:0.2849 val_acc:81.0219 val_loss:0.4885
[10/0077] | train_loss:0.2894 val_acc:79.0754 val_loss:0.4985
[10/0078] | train_loss:0.2964 val_acc:82.4818 val_loss:0.4852
model is saved at epoch 78!![10/0079] | train_loss:0.2917 val_acc:81.0219 val_loss:0.4727
[10/0080] | train_loss:0.2758 val_acc:82.4818 val_loss:0.4769
[10/0081] | train_loss:0.2767 val_acc:80.292 val_loss:0.4844
[10/0082] | train_loss:0.2839 val_acc:80.7786 val_loss:0.4757
[10/0083] | train_loss:0.276 val_acc:81.5085 val_loss:0.4936
[10/0084] | train_loss:0.2786 val_acc:82.2384 val_loss:0.4827
[10/0085] | train_loss:0.2779 val_acc:81.7518 val_loss:0.4936
[10/0086] | train_loss:0.2689 val_acc:81.9951 val_loss:0.4659
[10/0087] | train_loss:0.2528 val_acc:80.292 val_loss:0.5094
[10/0088] | train_loss:0.2646 val_acc:80.5353 val_loss:0.5182
[10/0089] | train_loss:0.2696 val_acc:81.5085 val_loss:0.4964
[10/0090] | train_loss:0.2804 val_acc:80.292 val_loss:0.508
[10/0091] | train_loss:0.2801 val_acc:79.3187 val_loss:0.5601
[10/0092] | train_loss:0.2647 val_acc:81.2652 val_loss:0.5074
[10/0093] | train_loss:0.2533 val_acc:82.9684 val_loss:0.4968
model is saved at epoch 93!![10/0094] | train_loss:0.2517 val_acc:81.2652 val_loss:0.4892
[10/0095] | train_loss:0.2497 val_acc:81.2652 val_loss:0.5232
[10/0096] | train_loss:0.2629 val_acc:81.9951 val_loss:0.4898
[10/0097] | train_loss:0.252 val_acc:81.5085 val_loss:0.4985
[10/0098] | train_loss:0.2383 val_acc:81.7518 val_loss:0.5623
[10/0099] | train_loss:0.2379 val_acc:80.292 val_loss:0.5494
[10/0100] | train_loss:0.2699 val_acc:79.562 val_loss:0.5223
[10/0101] | train_loss:0.2519 val_acc:82.4818 val_loss:0.5231
[10/0102] | train_loss:0.2448 val_acc:81.7518 val_loss:0.5617
[10/0103] | train_loss:0.2409 val_acc:80.7786 val_loss:0.5499
[10/0104] | train_loss:0.2362 val_acc:81.0219 val_loss:0.5542
[10/0105] | train_loss:0.2446 val_acc:80.292 val_loss:0.541
[10/0106] | train_loss:0.2294 val_acc:81.2652 val_loss:0.5638
[10/0107] | train_loss:0.2524 val_acc:81.2652 val_loss:0.5202
[10/0108] | train_loss:0.2347 val_acc:80.7786 val_loss:0.5503
[10/0109] | train_loss:0.2426 val_acc:80.7786 val_loss:0.5511
[10/0110] | train_loss:0.2431 val_acc:80.5353 val_loss:0.5543
[10/0111] | train_loss:0.2356 val_acc:80.0487 val_loss:0.5638
[10/0112] | train_loss:0.232 val_acc:79.0754 val_loss:0.5833
[10/0113] | train_loss:0.2568 val_acc:80.7786 val_loss:0.5408
[10/0114] | train_loss:0.2587 val_acc:80.0487 val_loss:0.5305
[10/0115] | train_loss:0.2212 val_acc:78.3455 val_loss:0.5779
[10/0116] | train_loss:0.2123 val_acc:82.4818 val_loss:0.5485
[10/0117] | train_loss:0.2145 val_acc:80.7786 val_loss:0.5527
[10/0118] | train_loss:0.2182 val_acc:82.2384 val_loss:0.5451
[10/0119] | train_loss:0.2344 val_acc:82.2384 val_loss:0.5367
[10/0120] | train_loss:0.2121 val_acc:80.5353 val_loss:0.5834
[10/0121] | train_loss:0.227 val_acc:80.5353 val_loss:0.555
[10/0122] | train_loss:0.2114 val_acc:80.7786 val_loss:0.5599
[10/0123] | train_loss:0.2259 val_acc:80.7786 val_loss:0.5616
[10/0124] | train_loss:0.2126 val_acc:80.0487 val_loss:0.555
[10/0125] | train_loss:0.2167 val_acc:80.0487 val_loss:0.5531
[10/0126] | train_loss:0.2203 val_acc:80.7786 val_loss:0.6437
[10/0127] | train_loss:0.2175 val_acc:79.3187 val_loss:0.5653
[10/0128] | train_loss:0.219 val_acc:80.5353 val_loss:0.6047
[10/0129] | train_loss:0.2224 val_acc:81.2652 val_loss:0.5701
[10/0130] | train_loss:0.211 val_acc:81.5085 val_loss:0.5966
[10/0131] | train_loss:0.2079 val_acc:81.5085 val_loss:0.5754
[10/0132] | train_loss:0.2121 val_acc:81.7518 val_loss:0.5615
[10/0133] | train_loss:0.2043 val_acc:79.562 val_loss:0.5876
[10/0134] | train_loss:0.2345 val_acc:81.0219 val_loss:0.5385
[10/0135] | train_loss:0.195 val_acc:81.7518 val_loss:0.5872
[10/0136] | train_loss:0.2136 val_acc:80.7786 val_loss:0.5663
[10/0137] | train_loss:0.201 val_acc:81.9951 val_loss:0.5973
[10/0138] | train_loss:0.202 val_acc:80.292 val_loss:0.6172
[10/0139] | train_loss:0.1957 val_acc:79.0754 val_loss:0.6244
[10/0140] | train_loss:0.2058 val_acc:81.2652 val_loss:0.6027
[10/0141] | train_loss:0.1948 val_acc:80.5353 val_loss:0.6266
[10/0142] | train_loss:0.1954 val_acc:79.0754 val_loss:0.6197
[10/0143] | train_loss:0.1982 val_acc:79.0754 val_loss:0.616
[10/0144] | train_loss:0.2033 val_acc:80.292 val_loss:0.5779
Fold: [10/10] Test is finish !! 
 Test Metrics are: test_acc:80.7786 test_loss:0.504
all fold acc is: 
[85.40145754814148, 76.88564658164978, 81.2652051448822, 81.02189898490906, 84.42822098731995, 81.2652051448822, 80.04866242408752, 83.21167826652527, 80.04866242408752, 80.77858686447144] 
Test is finish !! 
 Test Metrics are: acc_mean:81.4355 acc_std:2.3032